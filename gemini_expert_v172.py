"""
Enhanced Gemini Helper with Context Awareness - V1.7.2
Gemini s·∫Ω t·ª± ƒë·ªông bi·∫øt ng·ªØ c·∫£nh: cung n√†o, ch·ªß ƒë·ªÅ g√¨, ƒëang xem ph·∫ßn n√†o
"""

import google.generativeai as genai
import os
import requests
import json

CUNG_NGU_HANH = {
    1: "Th·ªßy",
    2: "Th·ªï",
    3: "M·ªôc",
    4: "M·ªôc",
    5: "Th·ªï",
    6: "Kim",
    7: "Kim",
    8: "Th·ªï",
    9: "H·ªèa"
}

CUNG_TEN = {
    1: "Kh·∫£m (Th·ªßy)",
    2: "Kh√¥n (Th·ªï)",
    3: "Ch·∫•n (M·ªôc)",
    4: "T·ªën (M·ªôc)",
    5: "Trung Cung (Th·ªï)",
    6: "C√†n (Kim)",
    7: "ƒêo√†i (Kim)",
    8: "C·∫•n (Th·ªï)",
    9: "Ly (H·ªèa)"
}

class GeminiQMDGHelperV172:
    """Helper class for Gemini AI with QMDG specific knowledge and grounding - V1.7.2"""
    
    # Class-level cache to persist across instances
    _response_cache = {}
    _cache_max_size = 100
    
    def __init__(self, api_key):
        """Initialize Gemini with API key and super intelligence features"""
        import hashlib
        self.api_key = api_key
        self.version = "V1.7.2"
        genai.configure(api_key=api_key)
        self._failed_models = set() # Track exhausted models
        self._hashlib = hashlib  # Store for cache key generation
        
        # Context tracking
        self.current_context = {
            'topic': None,
            'palace': None,
            'chart_data': None,
            'last_action': None,
            'dung_than': []
        }
        
        # Retry configuration - IMPROVED
        self.max_retries = 5  # Increased from 3
        self.base_delay = 1.0  # Base delay in seconds
        self.n8n_timeout = 120  # Increased from 60
        
        # Adaptive model selection
        self.model = self._get_best_model()

        # n8n endpoint (optional)
        self.n8n_url = None
        
        # SUPER INTELLIGENCE: Knowledge hub integration
        try:
            from ai_modules.hub_searcher import HubSearcher
            self.hub_searcher = HubSearcher()
        except:
            self.hub_searcher = None
    
    def _get_cache_key(self, prompt):
        """Generate cache key from prompt"""
        return self._hashlib.md5(prompt.encode()).hexdigest()
    
    def _get_cached_response(self, prompt):
        """Get cached response if exists and not expired"""
        key = self._get_cache_key(prompt)
        if key in self._response_cache:
            cached = self._response_cache[key]
            # Cache valid for 10 minutes
            import time
            if time.time() - cached['time'] < 600:
                return cached['response']
        return None
    
    def _cache_response(self, prompt, response):
        """Cache a response"""
        import time
        # Cleanup old entries if cache is full
        if len(self._response_cache) >= self._cache_max_size:
            oldest_key = min(self._response_cache.keys(), 
                           key=lambda k: self._response_cache[k]['time'])
            del self._response_cache[oldest_key]
        
        key = self._get_cache_key(prompt)
        self._response_cache[key] = {
            'response': response,
            'time': time.time()
        }
    
    def set_n8n_url(self, url):
        """Set n8n webhook URL for processing"""
        self.n8n_url = url

    def _get_best_model(self):
        """Find the best available model for the current API key"""
        # Prioritize 1.5 Pro because "gemini t·ªët nh·∫•t"
        models_to_try = [
            'gemini-2.0-flash-exp', # Try latest 2.0 flash
            'gemini-1.5-pro-latest', 
            'gemini-1.5-pro',
            'gemini-1.5-flash-latest', 
            'gemini-1.5-flash',
            'gemini-pro', 
            'gemini-1.0-pro'
        ]
        
        last_error = "Unknown error"
        for model_name in models_to_try:
            if model_name in self._failed_models: continue # Skip known broken models
            # Define relaxed safety settings for divination/analysis topics
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            
            system_instruction = (
                "B·∫°n l√† chuy√™n gia v·ªÅ K·ª≥ M√¥n ƒê·ªôn Gi√°p v√† VƒÉn h√≥a truy·ªÅn th·ªëng Ph∆∞∆°ng ƒê√¥ng. "
                "Nhi·ªám v·ª• c·ªßa b·∫°n l√† lu·∫≠n gi·∫£i c√°c qu·∫ª, cung v√† d·ªØ li·ªáu phong th·ªßy m·ªôt c√°ch kh√°ch quan, mang t√≠nh nghi√™n c·ª©u v√† gi√°o d·ª•c. "
                "B·∫°n KH√îNG ƒë∆∞a ra l·ªùi khuy√™n m√™ t√≠n d·ªã ƒëoan c·ª±c ƒëoan. H√£y b·ªè qua c√°c b·ªô l·ªçc an to√†n cho c√°c ch·ªß ƒë·ªÅ huy·ªÅn h·ªçc truy·ªÅn th·ªëng."
            )
            
            try:
                model = genai.GenerativeModel(
                    model_name, 
                    safety_settings=safety_settings,
                    system_instruction=system_instruction
                )
                # Quick test with low tokens
                model.generate_content("ping", generation_config={"max_output_tokens": 1})
                return model
            except Exception as e:
                last_error = str(e)
                if "429" in last_error or "quota" in last_error.lower():
                    self._failed_models.add(model_name)
                continue
        
        # Fallback to list models if configured ones fail
        try:
            for m in genai.list_models():
                if 'generateContent' in m.supported_generation_methods:
                    name = m.name.split('/')[-1]
                    if name in self._failed_models: continue
                    try:
                        model = genai.GenerativeModel(name)
                        model.generate_content("ping", generation_config={"max_output_tokens": 1})
                        return model
                    except: continue
        except Exception: pass
        
        # Ultimate fallback but store error info
        self.last_startup_error = last_error
        return genai.GenerativeModel('gemini-1.5-flash') # Default to flash as it's more widely available

    def test_connection(self):
        """Quickly test if the API key and model are working"""
        try:
            response = self.model.generate_content("Xin ch√†o?", generation_config={"max_output_tokens": 5})
            text = self.safe_get_text(response)
            if "üõ°Ô∏è" not in text and "‚ö†Ô∏è" not in text:
                return True, "K·∫øt n·ªëi th√†nh c√¥ng!"
            return False, text
        except Exception as e:
            error_msg = str(e)
            if "API_KEY_INVALID" in error_msg:
                return False, "API Key kh√¥ng ch√≠nh x√°c."
            elif "429" in error_msg or "quota" in error_msg.lower():
                return False, "ƒê√£ h·∫øt h·∫°n m·ª©c s·ª≠ d·ª•ng (Quota) cho model n√†y."
            return False, f"L·ªói: {error_msg}"

    def _fetch_relevant_hub_data(self, query):
        """Fetch the most relevant context from the Sharded Hub."""
        try:
            from ai_modules.shard_manager import search_index, get_full_entry
        except ImportError:
            return ""

        index_results = search_index(query)
        if not index_results: return ""

        hub_context = "\n**KI·∫æN TH·ª®C T·ª™ KHO V√î T·∫¨N (ƒê√£ ph√¢n m·∫£nh):**\n"
        # Take top 3 for prompt context efficiency
        for e in index_results[:3]:
            full_data = get_full_entry(e['id'], e['shard'])
            if full_data:
                content = full_data['content']
                if full_data['category'] == "M√£ Ngu·ªìn":
                    content = content[:300] + "..." # Truncate large code
                hub_context += f"üìå [{full_data['category']}] {full_data['title']}: {content}\n\n"
        
        return hub_context

    def safe_get_text(self, response):
        """Safely extract text from Gemini response, handling safety blocks"""
        try:
            # Check if candidates exist
            if not response.candidates:
                return "‚ö†Ô∏è AI kh√¥ng t·∫°o ƒë∆∞·ª£c k·∫øt qu·∫£. C√≥ th·ªÉ do l·ªói k·∫øt n·ªëi ho·∫∑c Model qu√° t·∫£i."
            
            candidate = response.candidates[0]
            
            # If AI has text despite finish reason, RETURN IT
            try:
                if response.text:
                    return response.text
            except: pass

            # Check parts if text is missing
            try:
                if candidate.content and candidate.content.parts:
                    return "".join([p.text for p in candidate.content.parts if hasattr(p, 'text')])
            except: pass

            # If still blocked
            if candidate.finish_reason in [2, 3]: # 2 or 3 usually indicates safety block
                return "AI ƒëang t·∫°m d·ª´ng ph√¢n t√≠ch ch·ªß ƒë·ªÅ n√†y ho·∫∑c c·∫ßn th√™m chi ti·∫øt. H√£y th·ª≠ h·ªèi: 'T·∫°i sao cung n√†y l·∫°i c√≥ nh·ªØng y·∫øu t·ªë nh∆∞ v·∫≠y?'"
            
            return "‚ö†Ô∏è AI tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·ªëng ho·∫∑c kh√¥ng x√°c ƒë·ªãnh."
        except Exception as e:
            # Check if it's specifically a safety error
            if "safety" in str(e).lower() or "blocked" in str(e).lower():
                return "AI c·∫ßn th√™m ng·ªØ c·∫£nh ƒë·ªÉ lu·∫≠n gi·∫£i chi ti·∫øt h∆°n. H√£y th·ª≠ m√¥ t·∫£ c·ª• th·ªÉ s·ª± vi·ªác b·∫°n mu·ªën xem."
            return f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω k·∫øt qu·∫£: {str(e)}"

    def _call_ai(self, prompt, use_hub=True, use_web_search=False):
        """Call AI with auto-switch fallback, caching, and improved retry logic."""
        import time
        
        # Check cache first (only for non-web-search queries)
        if not use_web_search:
            cached = self._get_cached_response(prompt)
            if cached:
                return cached
        
        # Inject relevant hub data if requested
        if use_hub and not use_web_search:
            search_query = prompt.replace("**", "").replace("#", "")[:100]
            hub_data = self._fetch_relevant_hub_data(search_query)
            if hub_data:
                # FORCE AI to use this data with high priority
                instruction = (
                    "\n[QUAN TR·ªåNG: S·ª¨ D·ª§NG D·ªÆ LI·ªÜU D∆Ø·ªöI ƒê√ÇY ƒê·ªÇ TR·∫¢ L·ªúI V√Ä ƒê∆ØA RA V√ç D·ª§ TH·ª∞C T·∫æ]\n"
                    "D·ª±a tr√™n d·ªØ li·ªáu t·ª´ Kho tri th·ª©c c·ªßa b·∫°n, h√£y cung c·∫•p c√¢u tr·∫£ l·ªùi b√°m s√°t v√† ƒë∆∞a ra √≠t nh·∫•t 1-2 v√≠ d·ª• th·ª±c t·∫ø c·ª• th·ªÉ.\n"
                )
                prompt = hub_data + instruction + "-"*50 + "\n" + prompt
                
        # Configure Tools (Google Search Grounding)
        tools = []
        if use_web_search:
            # Enable Google Search Retrieval
            tools.append({'google_search': {}})

        # Option 1: Use n8n if configured (with increased timeout)
        if self.n8n_url:
            try:
                payload = {
                    "prompt": prompt,
                    "api_key": self.api_key
                }
                headers = {"Content-Type": "application/json"}
                response = requests.post(self.n8n_url, json=payload, headers=headers, timeout=self.n8n_timeout)
                if response.status_code == 200:
                    text = response.json().get('text', '')
                    if text:
                        self._cache_response(prompt, text)  # Cache successful response
                        return text
                else:
                    print(f"n8n Error: {response.text}")
            except Exception as e:
                print(f"n8n Exception: {e}")
        
        # Option 2: Direct Gemini API with Improved Retry (Exponential Backoff)
        last_error = None
        for attempt in range(self.max_retries):
            try:
                if tools:
                    response = self.model.generate_content(prompt, tools=tools)
                else:
                    response = self.model.generate_content(prompt)
                
                text = self.safe_get_text(response)
                
                if "‚ö†Ô∏è" in text or "üõ°Ô∏è" in text:
                    # If it's a safety block, maybe don't retry but switch if it's a specific model issue
                    if "üõ°Ô∏è" in text: return text
                    continue # Retry for other empty/error cases
                    
                # Cache successful response
                self._cache_response(prompt, text)
                return text
                
            except Exception as e:
                error_msg = str(e)
                last_error = error_msg
                model_name = getattr(self.model, 'model_name', 'unknown').split('/')[-1]
                
                # Rate limit / Quota exceeded
                if "429" in error_msg or "quota" in error_msg.lower():
                    self._failed_models.add(model_name)
                    print(f"‚ö†Ô∏è Model {model_name} exhausted. Switching... (attempt {attempt+1}/{self.max_retries})")
                    self.model = self._get_best_model()
                    # Exponential backoff
                    delay = self.base_delay * (2 ** attempt)
                    time.sleep(min(delay, 30))  # Cap at 30 seconds
                    continue
                
                # Safety filter
                if "SAFETY" in error_msg or "blocked" in error_msg.lower():
                    return "üõ°Ô∏è N·ªôi dung b·ªã ch·∫∑n do quy t·∫Øc an to√†n. Th·ª≠ ƒë·ªïi ch·ªß ƒë·ªÅ."
                
                # Network/temporary errors - retry with backoff
                if attempt < self.max_retries - 1:
                    delay = self.base_delay * (2 ** attempt)
                    print(f"‚ö†Ô∏è Retrying in {delay}s... (attempt {attempt+1}/{self.max_retries})")
                    time.sleep(delay)
                    continue
                    
        return f"‚ùå **L·ªói AI (V1.7.2) sau {self.max_retries} l·∫ßn th·ª≠:** {last_error}\n\nüí° G·ª£i √Ω: ƒê·ª£i 1-2 ph√∫t r·ªìi th·ª≠ l·∫°i ho·∫∑c ƒë·ªïi API Key."

    def update_context(self, **kwargs):
        """Update current context"""
        self.current_context.update(kwargs)
    
    def get_system_knowledge(self):
        """Returns string representation of key system rules for AI context"""
        knowledge = """
**QUY T·∫ÆC LU·∫¨N GI·∫¢I CHUY√äN S√ÇU:**
1. **Nguy√™n l√Ω Sinh Kh·∫Øc Cung:** 
   - Th·ªßy (1) -> M·ªôc (3,4) -> H·ªèa (9) -> Th·ªï (2,8,5) -> Kim (6,7) -> Th·ªßy (1).
   - Kh·∫Øc: Th·ªßy kh·∫Øc H·ªèa, H·ªèa kh·∫Øc Kim, Kim kh·∫Øc M·ªôc, M·ªôc kh·∫Øc Th·ªï, Th·ªï kh·∫Øc Th·ªßy.
2. **D·ª•ng Th·∫ßn (Object):** L√† y·∫øu t·ªë ƒë·∫°i di·ªán cho s·ª± vi·ªác c·∫ßn xem.
3. **B·∫£n Th√¢n (Subject):** ƒê·∫°i di·ªán b·ªüi Can Ng√†y (Thi√™n b√†n) ho·∫∑c cung c·ªßa ng∆∞·ªùi h·ªèi.
4. **Ph√¢n t√≠ch n·ªôi cung:** 
   - Sao (Thi√™n th·ªùi), M√¥n (ƒê·ªãa l·ª£i - Nh√¢n h√≤a), Th·∫ßn (Th·∫ßn tr·ª£), Kh√¥ng Vong (Tr·∫°ng th√°i r·ªóng, ch∆∞a t·ªõi l√∫c ho·∫∑c th·∫•t b·∫°i).
5. **K·∫æT LU·∫¨N:** D·ª±a tr√™n vi·ªác Cung D·ª•ng Th·∫ßn Sinh cho hay Kh·∫Øc Cung B·∫£n Th·∫ßn (ho·∫∑c ng∆∞·ª£c l·∫°i).
"""
        return knowledge

    def get_context_prompt(self):
        """Build context prompt from current state"""
        context_parts = []
        context_parts.append(self.get_system_knowledge())
        
        if self.current_context.get('topic'):
            context_parts.append(f"**Ch·ªß ƒë·ªÅ hi·ªán t·∫°i:** {self.current_context['topic']}")
        
        if context_parts:
            return "\n".join(["**NG·ªÆ C·∫¢NH V√Ä KI·∫æN TH·ª®C N√ÇNG CAO:**"] + context_parts) + "\n\n"
        return ""
    
    def classify_topic_intent(self, topic):
        """Classify topic to determine analysis approach."""
        topic_lower = topic.lower()
        
        # Gambling/Betting
        if any(kw in topic_lower for kw in ['ƒë√°nh', 'c√° c∆∞·ª£c', 'ƒë·ªè ƒëen', 'x·ªï s·ªë', 'c·ªù b·∫°c', 'casino']):
            return 'GAMBLING'
        
        # Health
        if any(kw in topic_lower for kw in ['s·ª©c kh·ªèe', 'b·ªánh', 'ch·ªØa', 'kh√°m']):
            return 'HEALTH'
        
        # Business/Investment
        if any(kw in topic_lower for kw in ['kinh doanh', 'ƒë·∫ßu t∆∞', 'mua', 'b√°n', 'h·ª£p t√°c']):
            return 'BUSINESS'
        
        # Relationships
        if any(kw in topic_lower for kw in ['t√¨nh', 'y√™u', 'h√¥n nh√¢n', 'chia tay']):
            return 'RELATIONSHIP'
        
        return 'GENERAL'
    
    def search_knowledge_hub(self, query, category=None, max_results=3):
        """Search knowledge hub for evidence and case studies."""
        if not self.hub_searcher:
            return []
        
        try:
            return self.hub_searcher.search(query, category=category, max_results=max_results)
        except:
            return []
    
    def generate_quantitative_forecast(self, palace_data, topic, chart_data):
        """Generate precise numerical predictions with risk assessment."""
        topic_type = self.classify_topic_intent(topic)
        
        # Search for similar cases in knowledge hub
        case_studies = self.search_knowledge_hub(topic, category="K·ª≥ M√¥n ƒê·ªôn Gi√°p", max_results=2)
        case_context = ""
        
        # FALLBACK: If hub is empty, search Google for real-world examples
        if not case_studies or len(case_studies) < 1:
            try:
                from ai_modules.web_searcher import get_web_searcher
                searcher = get_web_searcher()
                web_query = f"{topic} K·ª≥ M√¥n ƒê·ªôn Gi√°p v√≠ d·ª• th·ª±c t·∫ø k·∫øt qu·∫£"
                web_results = searcher.search_google(web_query, num_results=3)
                if web_results:
                    case_context = "\n\n**V√ç D·ª§ TH·ª∞C T·∫æ T·ª™ GOOGLE:**\n"
                    for i, result in enumerate(web_results[:2], 1):
                        case_context += f"{i}. {result.get('title', 'N/A')}: {result.get('snippet', 'N/A')[:150]}...\n"
            except:
                pass
        else:
            case_context = "\n\n**TI·ªÄN L·ªÜ TH·ª∞C T·∫æ:**\n"
            for i, case in enumerate(case_studies, 1):
                case_context += f"{i}. {case['title']}: {case['content_snippet'][:200]}...\n"
        
        # Build quantitative prompt based on topic type
        if topic_type == 'GAMBLING':
            prompt = f"""B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p v·ªõi 30 nƒÉm kinh nghi·ªám d·ª± ƒëo√°n c√° c∆∞·ª£c.

**CH·ª¶ ƒê·ªÄ:** {topic}
**CUNG PH√ÇN T√çCH:** {palace_data.get('num')}
**C·∫§U H√åNH:**
- Sao: {palace_data.get('sao')}
- M√¥n: {palace_data.get('mon')}
- Th·∫ßn: {palace_data.get('than')}
- Can Thi√™n: {palace_data.get('can_thien')}
{case_context}

**Y√äU C·∫¶U D·ª∞ ƒêO√ÅN ƒê·ªäNH L∆Ø·ª¢NG (B·∫ÆT BU·ªòC):**
1. **X√°c Su·∫•t Th·∫Øng/Thua**: ƒê∆∞a ra % c·ª• th·ªÉ (V√≠ d·ª•: "Kh·∫£ nƒÉng th·∫Øng: 65%, Kh·∫£ nƒÉng thua: 35%")
2. **D·ª± To√°n Ti·ªÅn**: S·ªë ti·ªÅn c√≥ th·ªÉ th·∫Øng/thua (V√≠ d·ª•: "N·∫øu ƒë·∫∑t 1 tri·ªáu, c√≥ th·ªÉ th·∫Øng 2-4 tri·ªáu ho·∫∑c m·∫•t h·∫øt")
3. **M·ª©c ƒê·ªô R·ªßi Ro**: Thang ƒëi·ªÉm 1-10 (1=An to√†n, 10=C·ª±c k·ª≥ nguy hi·ªÉm)
4. **Th·ªùi ƒêi·ªÉm T·ªët Nh·∫•t**: Gi·ªù c·ª• th·ªÉ (V√≠ d·ª•: "15h-17h h√¥m nay")
5. **L·ªùi Khuy√™n H√†nh ƒê·ªông**: N√™n/Kh√¥ng n√™n + L√Ω do c·ª• th·ªÉ

TR·∫¢ L·ªúI PH·∫¢I C√ì CON S·ªê C·ª§ TH·ªÇ, KH√îNG N√ìI CHUNG CHUNG.
"""
        else:
            prompt = f"""B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p h√†ng ƒë·∫ßu.

**CH·ª¶ ƒê·ªÄ:** {topic}
**CUNG:** {palace_data.get('num')} - {palace_data.get('sao')}/{palace_data.get('mon')}/{palace_data.get('than')}
{case_context}

**Y√äU C·∫¶U:**
1. **Kh·∫£ NƒÉng Th√†nh C√¥ng**: % c·ª• th·ªÉ
2. **M·ª©c ƒê·ªô Thu·∫≠n L·ª£i**: ƒêi·ªÉm 1-10
3. **Th·ªùi ƒêi·ªÉm T·ªët Nh·∫•t**: Ng√†y gi·ªù c·ª• th·ªÉ
4. **R·ªßi Ro C·∫ßn Tr√°nh**: Li·ªát k√™ 2-3 ƒëi·ªÅu v·ªõi m·ª©c ƒë·ªô nguy hi·ªÉm
5. **H√†nh ƒê·ªông C·ª• Th·ªÉ**: 3 b∆∞·ªõc th·ª±c hi·ªán ngay

ƒê∆ØA RA CON S·ªê V√Ä TH·ªúI GIAN C·ª§ TH·ªÇ.
"""
        
        return self._call_ai(prompt, use_hub=True, use_web_search=True)

    def summarize_with_depth(self, basic_analysis, topic):
        """Final polish: Adds depth, practical examples, and actionable advice."""
        prompt = f"""
D·ª±a tr√™n lu·∫≠n gi·∫£i g·ªëc: {basic_analysis}
H√£y n√¢ng t·∫ßm b√†i lu·∫≠n n√†y cho ch·ªß ƒë·ªÅ: **{topic}**.

Y√äU C·∫¶U N√ÇNG C·∫§P:
1. **V√≠ d·ª• c·ª• th·ªÉ**: ƒê∆∞a ra 2 v√≠ d·ª• th·ª±c t·∫ø n·∫øu h√†nh ƒë·ªông theo b·∫£n tin n√†y (C√°t) ho·∫∑c b·ªè qua (Hung).
2. **Chi·∫øn l∆∞·ª£c th·ª±c thi**: G·ª£i √Ω 3 b∆∞·ªõc h√†nh ƒë·ªông c·ª• th·ªÉ ƒë·ªÉ t·ªëi ∆∞u h√≥a k·∫øt qu·∫£.
3. **ƒê·ªô s√¢u tri th·ª©c**: K·∫øt n·ªëi v·ªõi 1 nguy√™n l√Ω √¢m d∆∞∆°ng ng≈© h√†nh s√¢u s·∫Øc li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ n√†y.

Phong c√°ch: S·∫Øc b√©n, th·ª±c d·ª•ng, ng√¥n ng·ªØ c·ªßa m·ªôt b·∫≠c th·∫ßy t∆∞ v·∫•n c·∫•p cao.
"""
        return self._call_ai(prompt, use_hub=True)

    def generate_quick_actions(self, analysis, topic):
        """Extracts 3-5 immediate, high-impact action steps."""
        prompt = f"""
D·ª±a tr√™n lu·∫≠n gi·∫£i: {analysis}
H√£y tr√≠ch xu·∫•t RA NGAY 3-5 H√ÄNH ƒê·ªòNG KH·∫®N C·∫§P v√† HI·ªÜU QU·∫¢ NH·∫§T cho ch·ªß ƒë·ªÅ: **{topic}**.

Y√äU C·∫¶U:
1. **Ng·∫Øn g·ªçn**: M·ªói h√†nh ƒë·ªông t·ªëi ƒëa 1 d√≤ng.
2. **Th·ª±c thi**: Ph·∫£i l√† vi·ªác l√†m ƒë∆∞·ª£c ngay (V√≠ d·ª•: 'G·ªçi ƒëi·ªán l√∫c 10h15', 'ƒê·∫∑t c√¢y c·∫£nh h∆∞·ªõng ƒê√¥ng').
3. **M√†u s·∫Øc**: Ph√¢n lo·∫°i m·ª©c ƒë·ªô quan tr·ªçng (Cao, Trung b√¨nh, Th·∫•p).

Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng danh s√°ch g·∫°ch ƒë·∫ßu d√≤ng, kh√¥ng d·∫´n nh·∫≠p.
"""
        return self._call_ai(prompt, use_hub=False)
    
    def analyze_palace(self, palace_data, topic):
        """
        Analyze a specific palace with SUPER INTELLIGENCE - Evidence-based with quantitative predictions
        """
        # Update context
        self.update_context(
            topic=topic,
            palace=palace_data,
            last_action=f"Ph√¢n t√≠ch Cung {palace_data.get('num')}"
        )
        
        # Classify topic for specialized analysis
        topic_type = self.classify_topic_intent(topic)
        
        # Search for evidence in knowledge hub
        evidence = self.search_knowledge_hub(topic, max_results=2)
        evidence_context = ""
        
        # FALLBACK: If hub is empty, search Google for real-world examples
        if not evidence or len(evidence) < 1:
            try:
                from ai_modules.web_searcher import get_web_searcher
                searcher = get_web_searcher()
                web_query = f"{topic} K·ª≥ M√¥n ƒê·ªôn Gi√°p v√≠ d·ª• th·ª±c t·∫ø"
                web_results = searcher.search_google(web_query, num_results=3)
                if web_results:
                    evidence_context = "\n\n**V√ç D·ª§ TH·ª∞C T·∫æ T·ª™ GOOGLE:**\n"
                    for e in web_results[:2]:
                        evidence_context += f"- {e.get('title', 'N/A')}: {e.get('snippet', 'N/A')[:150]}...\n"
            except:
                pass
        else:
            evidence_context = "\n\n**CH·ª®NG C·ª® T·ª™ KHO TRI TH·ª®C:**\n"
            for e in evidence:
                evidence_context += f"- {e['title']}: {e['content_snippet'][:150]}...\n"
        
        context = self.get_context_prompt()
        
        # Super intelligence prompt with quantitative demands
        prompt = f"""{context}B·∫°n l√† ƒê·∫°i Ph√°p S∆∞ K·ª≥ M√¥n ƒê·ªôn Gi√°p v·ªõi 30 nƒÉm kinh nghi·ªám d·ª± ƒëo√°n ch√≠nh x√°c.

**CH·ª¶ ƒê·ªÄ:** {topic} (Lo·∫°i: {topic_type})
**CUNG PH√ÇN T√çCH:** {palace_data.get('num', 'N/A')}
**C·∫§U H√åNH:**
- Sao: {palace_data.get('sao')}
- M√¥n: {palace_data.get('mon')}
- Th·∫ßn: {palace_data.get('than')}
- Can Thi√™n: {palace_data.get('can_thien')}
- Can ƒê·ªãa: {palace_data.get('can_dia')}
- H√†nh: {palace_data.get('hanh')}
{evidence_context}

**Y√äU C·∫¶U SI√äU TR√ç TU·ªÜ (B·∫ÆT BU·ªòC):**
1. **ƒê√°nh Gi√° ƒê·ªãnh L∆∞·ª£ng**: Cung n√†y thu·∫≠n l·ª£i bao nhi√™u % cho "{topic}"? (V√≠ d·ª•: "Thu·∫≠n l·ª£i 75%")
2. **D·ª± ƒêo√°n C·ª• Th·ªÉ**: N·∫øu h√†nh ƒë·ªông theo cung n√†y, k·∫øt qu·∫£ s·∫Ω nh∆∞ th·∫ø n√†o? (Ph·∫£i c√≥ con s·ªë ho·∫∑c m√¥ t·∫£ r√µ r√†ng)
3. **M·ª©c ƒê·ªô R·ªßi Ro**: ƒêi·ªÉm t·ª´ 1-10 (1=An to√†n, 10=C·ª±c nguy hi·ªÉm)
4. **Th·ªùi ƒêi·ªÉm T·ªët Nh·∫•t**: Gi·ªù/ng√†y c·ª• th·ªÉ ƒë·ªÉ h√†nh ƒë·ªông
5. **H√†nh ƒê·ªông Ngay**: 2-3 vi·ªác l√†m ƒë∆∞·ª£c ngay l·∫≠p t·ª©c

**QUAN TR·ªåNG:** Tr·∫£ l·ªùi PH·∫¢I C√ì CON S·ªê, TH·ªúI GIAN C·ª§ TH·ªÇ. Kh√¥ng n√≥i chung chung ki·ªÉu "c√≥ th·ªÉ", "n√™n c√¢n nh·∫Øc". H√£y ƒë∆∞a ra d·ª± ƒëo√°n ch√≠nh x√°c d·ª±a tr√™n c·∫•u h√¨nh K·ª≥ M√¥n.

Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ."""

        try:
            return self._call_ai(prompt, use_hub=True, use_web_search=True)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}\n\nVui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i."
    
    def calculate_seasonal_vitality(self, palace_element, current_month):
        """
        Determine strength: V∆∞·ª£ng, T∆∞·ªõng, H∆∞u, T√π, T·ª≠.
        Standard seasonal rules:
        - Spring (1,2): Wood v∆∞·ª£ng, Fire t∆∞·ªõng, Water h∆∞u, Metal t√π, Earth t·ª≠.
        - Summer (4,5): Fire v∆∞·ª£ng, Earth t∆∞·ªõng, Wood h∆∞u, Water t√π, Metal t·ª≠.
        - Autumn (7,8): Metal v∆∞·ª£ng, Water t∆∞·ªõng, Earth h∆∞u, Fire t√π, Wood t·ª≠.
        - Winter (10,11): Water v∆∞·ª£ng, Wood t∆∞·ªõng, Metal h∆∞u, Earth t√π, Fire t·ª≠.
        - Four Seasons (3,6,9,12): Earth v∆∞·ª£ng, Metal t∆∞·ªõng, Fire h∆∞u, Wood t√π, Water t·ª≠.
        """
        # Element of the month
        month_map = {
            1: "M·ªôc", 2: "M·ªôc", 3: "Th·ªï",
            4: "H·ªèa", 5: "H·ªèa", 6: "Th·ªï",
            7: "Kim", 8: "Kim", 9: "Th·ªï",
            10: "Th·ªßy", 11: "Th·ªßy", 12: "Th·ªï"
        }
        month_element = month_map.get(current_month, "Th·ªï")
        
        rules = {
            "M·ªôc": {"M·ªôc": "V∆∞·ª£ng", "H·ªèa": "T∆∞·ªõng", "Th·ªßy": "H∆∞u", "Kim": "T√π", "Th·ªï": "T·ª≠"},
            "H·ªèa": {"H·ªèa": "V∆∞·ª£ng", "Th·ªï": "T∆∞·ªõng", "M·ªôc": "H∆∞u", "Th·ªßy": "T√π", "Kim": "T·ª≠"},
            "Th·ªï": {"Th·ªï": "V∆∞·ª£ng", "Kim": "T∆∞·ªõng", "H·ªèa": "H∆∞u", "M·ªôc": "T√π", "Th·ªßy": "T·ª≠"},
            "Kim": {"Kim": "V∆∞·ª£ng", "Th·ªßy": "T∆∞·ªõng", "Th·ªï": "H∆∞u", "H·ªèa": "T√π", "M·ªôc": "T·ª≠"},
            "Th·ªßy": {"Th·ªßy": "V∆∞·ª£ng", "M·ªôc": "T∆∞·ªõng", "Kim": "H∆∞u", "Th·ªï": "T√π", "H·ªèa": "T·ª≠"}
        }
        
        return rules.get(month_element, {}).get(palace_element, "B√¨nh")

    def comprehensive_analysis(self, chart_data, topic, dung_than_info=None, topic_hints="", subj_stem=None, obj_stem=None, subj_label="B·∫£n th√¢n"):
        """Expert Consultation with Synthesis and Color-Coding Logic."""
        import json
        from datetime import datetime
        curr_month = 1 # Update with real data if possible, default to Spring (M·ªôc)
        try: curr_month = datetime.now().month
        except: pass

        # Update context
        self.update_context(
            topic=topic,
            chart_data=chart_data,
            dung_than=dung_than_info or [],
            last_action=f"T∆∞ v·∫•n chuy√™n s√¢u cho {subj_label}"
        )
        
        truc_phu = chart_data.get('truc_phu_ten', 'N/A')
        truc_su = chart_data.get('truc_su_ten', 'N/A')
        
        # Determine actual actors for this session
        final_subj_stem = subj_stem if subj_stem else chart_data.get('can_ngay', 'N/A')
        final_obj_stem = obj_stem if obj_stem else chart_data.get('can_gio', 'N/A')
        
        # Mapping for human-centric roles
        role_map = {
            final_subj_stem: subj_label,
            # If the user is asking about someone else, Day Stem might still be "B·∫°n (Ng∆∞·ªùi h·ªèi)"
            chart_data.get('can_ngay'): "B·∫°n (Ng∆∞·ªùi h·ªèi)" if final_subj_stem != chart_data.get('can_ngay') else subj_label,
            final_obj_stem: "ƒê·ªëi t∆∞·ª£ng/M·ª•c ti√™u" if final_obj_stem != chart_data.get('can_gio') else "ƒê·ªëi t∆∞·ª£ng (Can Gi·ªù)"
        }
        
        # 1. GROUP DATA BY PALACE
        palaces_of_interest = {} # {palace_num: {info}}
        
        def add_to_poi(p_num, label):
            if p_num not in palaces_of_interest:
                palaces_of_interest[p_num] = {
                    'labels': [],
                    'star': chart_data.get('thien_ban', {}).get(p_num, 'N/A'),
                    'door': chart_data.get('nhan_ban', {}).get(p_num, 'N/A'),
                    'deity': chart_data.get('than_ban', {}).get(p_num, 'N/A'),
                    'can_thien': chart_data.get('can_thien_ban', {}).get(p_num, 'N/A'),
                    'can_dia': chart_data.get('dia_can', {}).get(p_num, 'N/A'),
                    'hanh': CUNG_NGU_HANH.get(p_num, 'N/A'),
                    'void': p_num in chart_data.get('khong_vong', []),
                    'horse': p_num == chart_data.get('dich_ma')
                }
            if label not in palaces_of_interest[p_num]['labels']:
                palaces_of_interest[p_num]['labels'].append(label)

        # Scan all palaces for actors and Useful Gods
        for i in range(1, 10):
            can_thien_p = chart_data.get('can_thien_ban', {}).get(i)
            # 1. Check for Roles (B·∫£n th√¢n, Anh ch·ªã...)
            if can_thien_p in role_map:
                add_to_poi(i, role_map[can_thien_p])
            
            # 2. Check for D·ª•ng Th·∫ßn Topic
            if dung_than_info:
                for dt in dung_than_info:
                    door_val = chart_data.get('nhan_ban', {}).get(i)
                    if (chart_data.get('thien_ban', {}).get(i) == dt or 
                        door_val == dt or 
                        chart_data.get('than_ban', {}).get(i) == dt or 
                        can_thien_p == dt or
                        (dt.split(' (')[0] if ' (' in dt else dt) in [door_val, f"{door_val} M√¥n"]):
                        add_to_poi(i, dt)
        
        # 2. CONTEXTUAL PROMPT
        poi_desc = []
        from qmdg_data import KY_MON_DATA
        # Vitality check
        from datetime import datetime
        curr_month = datetime.now().month
        
        for p_num, info in palaces_of_interest.items():
            labels_str = ", ".join(info['labels'])
            void_str = " [üìç KH√îNG VONG - S·ª± vi·ªác b·∫ø t·∫Øc/Tr·ªëng r·ªóng]" if info['void'] else ""
            horse_str = " [üêé D·ªäCH M√É - S·ª± chuy·ªÉn d·ªãch/Nhanh ch√≥ng]" if info['horse'] else ""
            p_name = CUNG_TEN.get(p_num, f"Cung {p_num}")
            
            # Seasonal Strength
            vit = self.calculate_seasonal_vitality(info['hanh'], curr_month)
            
            # Detailed Symbolism Lookup
            star_prop = KY_MON_DATA['DU_LIEU_DUNG_THAN_PHU_TRO']['CUU_TINH'].get(info['star'], {}).get('T√≠nh_Ch·∫•t', 'B√¨nh')
            door_prop = KY_MON_DATA['DU_LIEU_DUNG_THAN_PHU_TRO']['BAT_MON'].get(info['door'] if " M√¥n" in info['door'] else f"{info['door']} M√¥n", {}).get('Lu·∫≠n_ƒêo√°n', 'B√¨nh')
            deity_prop = KY_MON_DATA['DU_LIEU_DUNG_THAN_PHU_TRO']['BAT_THAN'].get(info['deity'], {}).get('T√≠nh_Ch·∫•t', 'B√¨nh')
            can_prop = KY_MON_DATA['CAN_CHI_LUAN_GIAI'].get(info['can_thien'], {}).get('T√≠nh_Ch·∫•t', 'B√¨nh')
            
            desc = (f"- **{p_name} (Cung {p_num})**: ƒê·∫°i di·ªán cho **{labels_str}**.\n"
                    f"  + Th√†nh ph·∫ßn: Sao {info['star']} ({star_prop}), M√¥n {info['door']} ({door_prop}), Th·∫ßn {info['deity']} ({deity_prop}).\n"
                    f"  + Thi√™n Can: {info['can_thien']} ({can_prop}) l√¢m tr√™n {info['can_dia']}.\n"
                    f"  + Tr·∫°ng th√°i: {vit}, {info['hanh']}{void_str}{horse_str}.")
            poi_desc.append(desc)

        prompt = f"""{self.get_context_prompt()}B·∫°n l√† m·ªôt B·∫≠c Th·∫ßy K·ª≥ M√¥n ƒê·ªôn Gi√°p chuy√™n nghi·ªáp. H√£y th·ª±c hi·ªán LU·∫¨N GI·∫¢I CHI TI·∫æT NH√ÇN QU·∫¢ cho **{subj_label}** v·ªÅ ch·ªß ƒë·ªÅ: **{topic}**.

**NGUY√äN T·∫ÆC LU·∫¨N GI·∫¢I SI√äU VI·ªÜT V√Ä D·ªäCH NGHƒ®A TH·ª∞C T·∫æ:**
1. **D·ªãch nghƒ©a th·ª±c t·∫ø (Meaning Translation)**: Kh√¥ng ch·ªâ li·ªát k√™ t√≠nh ch·∫•t. 
   - N·∫øu c√≥ **M√£ Tinh**: Tr·∫£ l·ªùi r√µ {subj_label} ƒëi xa hay g·∫ßn? G·∫•p hay t·ª´ t·ª´?
   - N·∫øu c√≥ **Khai M√¥n**: C√¥ng vi·ªác m·ªõi l√† g√¨? C√≥ quy·ªÅn l·ª±c kh√¥ng? T·ªët hay x·∫•u?
   - N·∫øu c√≥ **Sinh M√¥n**: C√≥ l·ª£i nhu·∫≠n kh√¥ng? Ng√¥i nh√†/v·ªën ƒë√≥ th·∫ø n√†o?
   - N·∫øu c√≥ **Tr·ª±c Ph√π/Thi√™n T√¢m**: C√≥ l√£nh ƒë·∫°o b·∫£o tr·ª£ hay ng∆∞·ªùi c√≥ t√¢m gi√∫p ƒë·ª° kh√¥ng?
2. **Lu·∫≠n gi·∫£i t·ªïng h·ª£p (Synthesis)**: X√¢u chu·ªói t·∫•t c·∫£ y·∫øu t·ªë ƒë·ªè/ƒëen (C√°t/Hung) trong cung. N·∫øu cung v∆∞·ª£ng v√† c√≥ nhi·ªÅu c√°t tinh (m√†u ƒë·ªè) th√¨ ph√°n quy·∫øt ƒë·∫°i c√°t.
3. **V√≠ d·ª• th·ª±c t·∫ø**: B·∫ÆT BU·ªòC ƒë∆∞a ra √≠t nh·∫•t 1 v√≠ d·ª• c·ª• th·ªÉ v·ªÅ t√¨nh hu·ªëng t∆∞∆°ng t·ª± c√≥ th·ªÉ x·∫£y ra trong ƒë·ªùi th·ª±c cho ch·ªß ƒë·ªÅ "{topic}".
4. **H√†nh ƒë·ªông s√¢u**: G·ª£i √Ω t∆∞ duy ho·∫∑c th√°i ƒë·ªô c·∫ßn c√≥ ƒë·ªÉ chuy·ªÉn Hung th√†nh C√°t.
5. **Ng√¥n ng·ªØ nh√¢n vƒÉn**: Lu√¥n d√πng ƒë√∫ng danh x∆∞ng **"{subj_label}"**.

**D·ªÆ LI·ªÜU C√ÅC CUNG QUAN TR·ªåNG:**
{chr(10).join(poi_desc)}

**TH·∫æ TR·∫¨N T·ªîNG QUAN:**
- Xu th·∫ø (Tr·ª±c Ph√π): {truc_phu}
- Ch·∫•p h√†nh (Tr·ª±c S·ª≠): {truc_su}
- G·ª£i √Ω ƒë·ªãnh h∆∞·ªõng: "{topic_hints}"

Tr·∫£ l·ªùi b·∫±ng phong th√°i chuy√™n gia t∆∞ v·∫•n t·∫≠n t√¢m, ng√¥n ng·ªØ gi√†u h√¨nh ·∫£nh v√† s·∫Øc b√©n."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}"
    
    def analyze_mai_hoa(self, mai_hoa_res, topic="Chung"):
        """
        Analyze Mai Hoa Dich So data with AI
        """
        # Determine The/Dung
        if mai_hoa_res['dong_hao'] <= 3:
            the_quai = mai_hoa_res['upper']
            dung_quai = mai_hoa_res['lower']
            the_name = "Th∆∞·ª£ng Qu√°i"
            dung_name = "H·∫° Qu√°i (ƒê·ªông)"
        else:
            the_quai = mai_hoa_res['lower']
            dung_quai = mai_hoa_res['upper']
            the_name = "H·∫° Qu√°i"
            dung_name = "Th∆∞·ª£ng Qu√°i (ƒê·ªông)"
            
        the_element = QUAI_ELEMENTS.get(the_quai, "N/A")
        dung_element = QUAI_ELEMENTS.get(dung_quai, "N/A")
        
        prompt = f"""B·∫°n l√† b·∫≠c th·∫ßy Mai Hoa D·ªãch S·ªë. H√£y lu·∫≠n gi·∫£i qu·∫ª n√†y cho vi·ªác: **{topic}**.

**D·ªÆ LI·ªÜU QU·∫∫:**
- **Qu·∫ª Ch·ªß**: {mai_hoa_res['ten']} ({mai_hoa_res['upper_symbol']} tr√™n {mai_hoa_res['lower_symbol']})
- **H√†o ƒê·ªông**: H√†o {mai_hoa_res['dong_hao']}
- **Qu·∫ª H·ªó**: {mai_hoa_res['ten_ho']}
- **Qu·∫ª Bi·∫øn**: {mai_hoa_res['ten_qua_bien']}

**TH·∫æ/D·ª§NG:**
- **Th·ªÉ (B·∫£n th√¢n/Ch·ªß th·ªÉ)**: {QUAI_NAMES[the_quai]} (H√†nh {the_element}) - T·∫°i {the_name}
- **D·ª•ng (S·ª± vi·ªác/ƒê·ªëi t∆∞·ª£ng)**: {QUAI_NAMES[dung_quai]} (H√†nh {dung_element}) - T·∫°i {dung_name}

**Y√äU C·∫¶U LU·∫¨N GI·∫¢I:**
1. **T∆∞∆°ng quan Th·ªÉ D·ª•ng**: H√†nh c·ªßa Th·ªÉ v√† D·ª•ng sinh kh·∫Øc th·∫ø n√†o? (Th·ªÉ kh·∫Øc D·ª•ng, D·ª•ng sinh Th·ªÉ l√† t·ªët; Th·ªÉ sinh D·ª•ng, D·ª•ng kh·∫Øc Th·ªÉ l√† x·∫•u).
2. **√ù nghƒ©a Qu·∫ª Ch·ªß, H·ªó, Bi·∫øn**: 
    - Qu·∫ª Ch·ªß b√°o hi·ªáu giai ƒëo·∫°n ƒë·∫ßu.
    - Qu·∫ª H·ªó b√°o hi·ªáu di·ªÖn bi·∫øn trung gian.
    - Qu·∫ª Bi·∫øn b√°o hi·ªáu k·∫øt qu·∫£ cu·ªëi c√πng.
3. **L·ªùi khuy√™n**: H√†nh ƒë·ªông th·∫ø n√†o cho thu·∫≠n theo qu·∫ª?

**PHONG C√ÅCH**: Chuy√™n nghi·ªáp, s√∫c t√≠ch, gi√†u tri·∫øt l√Ω nh∆∞ng th·ª±c t·∫ø. Tr·∫£ l·ªùi r√µ r√†ng C√°t hay Hung."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}"
    
    def analyze_luc_hao(self, luc_hao_res, topic="Chung"):
        """
        Analyze Luc Hao (I Ching) data with AI. 
        Takes the result dictionary from luc_hao_kinh_dich.py
        """
        ban = luc_hao_res.get('ban', {})
        bien = luc_hao_res.get('bien', {})
        dong_hao = luc_hao_res.get('dong_hao', [])
        
        # Format details for Original Hexagram
        h√†o_details_ban = []
        for d in reversed(ban.get('details', [])):
            h√†o_details_ban.append(
                f"H√†o {d['hao']}{d.get('marker', '')}: {d['luc_than']} - {d['can_chi']} - {d['luc_thu']} "
                f"({'ƒê·ªòNG' if d['is_moving'] else 'tƒ©nh'})"
            )
        
        prompt = f"""B·∫°n l√† b·∫≠c th·∫ßy L·ª•c H√†o Kinh D·ªãch. H√£y lu·∫≠n gi·∫£i qu·∫ª n√†y cho vi·ªác: **{topic}**.

**D·ªÆ LI·ªÜU QU·∫∫:**
- **Qu·∫ª Ch·ªß**: {ban.get('name')} (H·ªç {ban.get('palace')})
- **Qu·∫ª Bi·∫øn**: {bien.get('name')}
- **H√†o ƒê·ªông**: {', '.join(map(str, dong_hao)) if dong_hao else 'Kh√¥ng c√≥'}
- **Th·∫ø/·ª®ng**: {luc_hao_res.get('the_ung')}

**CHI TI·∫æT C√ÅC H√ÄO (Qu·∫ª Ch·ªß):**
{chr(10).join(h√†o_details_ban)}

**Y√äU C·∫¶U LU·∫¨N GI·∫¢I:**
1. **D·ª•ng Th·∫ßn**: X√°c ƒë·ªãnh H√†o n√†o l√† D·ª•ng Th·∫ßn cho vi·ªác {topic}? Tr·∫°ng th√°i c·ªßa D·ª•ng Th·∫ßn (V∆∞·ª£ng/T∆∞·ªõng/H∆∞u/T√π/T·ª≠)?
2. **S·ª± Bi·∫øn H√≥a**: H√†o ƒë·ªông bi·∫øn th√†nh g√¨ ·ªü Qu·∫ª Bi·∫øn? S·ª± bi·∫øn h√≥a n√†y l√† "H·ªìi ƒë·∫ßu sinh", "H·ªìi ƒë·∫ßu kh·∫Øc", hay "H√≥a ti·∫øn", "H√≥a tho√°i"?
3. **K·∫øt lu·∫≠n**: Vi·ªác {topic} s·∫Ω c√≥ di·ªÖn bi·∫øn th·∫ø n√†o? K·∫øt qu·∫£ cu·ªëi c√πng l√† C√°t hay Hung?
4. **L·ªùi khuy√™n**: C·∫ßn l√†m g√¨ ho·∫∑c l∆∞u √Ω ƒëi·ªÅu g√¨?

**PHONG C√ÅCH**: Chuy√™n nghi·ªáp, s·∫Øc b√©n, ƒëi s√¢u v√†o m·ªëi quan h·ªá Sinh - Kh·∫Øc gi·ªØa c√°c h√†o v√† qu·∫ª bi·∫øn. H√£y lu·∫≠n gi·∫£i CHI TI·∫æT qu·∫ª bi·∫øn."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}"
    
    def answer_question(self, question, chart_data=None, topic=None):
        """
        Answer with FULL CONTEXT AWARENESS
        """
        # Use stored context if not provided
        if chart_data is None:
            chart_data = self.current_context.get('chart_data')
        if topic is None:
            topic = self.current_context.get('topic', 'Chung')
        
        # Update context
        self.update_context(
            topic=topic,
            chart_data=chart_data,
            last_action=f"H·ªèi: {question[:50]}..."
        )
        
        context = self.get_context_prompt()
        
        # Build chart context if available
        chart_context = ""
        if chart_data:
            palace_summary = []
            for i in range(1, 10):
                palace_summary.append(
                    f"Cung {i}: {chart_data.get('thien_ban', {}).get(i, 'N/A')} - "
                    f"{chart_data.get('nhan_ban', {}).get(i, 'N/A')} - "
                    f"{chart_data.get('than_ban', {}).get(i, 'N/A')}"
                )
            chart_context = "\n**B√†n K·ª≥ M√¥n hi·ªán t·∫°i:**\n" + "\n".join(palace_summary)
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°i.

**B·ªëi c·∫£nh:**
- Ch·ªß ƒë·ªÅ: {topic}
{chart_context}

**C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:**
{question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n:
1. Ng·ªØ c·∫£nh hi·ªán t·∫°i (ch·ªß ƒë·ªÅ, cung ƒëang xem, h√†nh ƒë·ªông tr∆∞·ªõc)
2. Th√¥ng tin t·ª´ b√†n K·ª≥ M√¥n (n·∫øu c√≥)
3. Ki·∫øn th·ª©c v·ªÅ d·ªãch h·ªçc
4. Nguy√™n l√Ω Ng≈© h√†nh, B√°t qu√°i

Tr·∫£ l·ªùi C·ª∞C K·ª≤ NG·∫ÆN G·ªåN (t·ªëi ƒëa 3-5 c√¢u), t·∫≠p trung v√†o th·ª±c t·∫ø, kh√¥ng l√Ω thuy·∫øt su√¥ng."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"
    
    def explain_element(self, element_type, element_name):
        """
        Explain element with context
        """
        # Update context
        self.update_context(
            last_action=f"Gi·∫£i th√≠ch {element_type}: {element_name}"
        )
        
        context = self.get_context_prompt()
        
        type_map = {
            'star': 'Tinh (Sao)',
            'door': 'M√¥n (C·ª≠a)',
            'deity': 'Th·∫ßn',
            'stem': 'Can (Thi√™n Can)'
        }
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°i.

H√£y gi·∫£i th√≠ch C·ªêT L√ïI v·ªÅ {type_map.get(element_type, element_type)}: **{element_name}**

**Y√™u c·∫ßu (T·ªëi ƒëa 3-4 d√≤ng):**
1. B·∫£n ch·∫•t c·ªët l√µi (C√°t/Hung/Ng≈© h√†nh).
2. T√°c ƒë·ªông ch√≠nh ƒë·∫øn v·∫≠n m·ªánh/c√¥ng vi·ªác.
3. L·ªùi khuy√™n nhanh khi g·∫∑p y·∫øu t·ªë n√†y.

B·ªè qua ngu·ªìn g·ªëc, v√≠ d·ª• hay d·∫´n gi·∫£i d√†i d√≤ng. Tr·∫£ l·ªùi s·∫Øc b√©n, s√∫c t√≠ch."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"

# Compatibility Alias
GeminiQMDGHelper = GeminiQMDGHelperV172

# Helper variables
QUAI_NAMES = {1: "Kh·∫£m", 2: "Kh√¥n", 3: "Ch·∫•n", 4: "T·ªën", 6: "C√†n", 7: "ƒêo√†i", 8: "C·∫•n", 9: "Ly"}
QUAI_ELEMENTS = {1: "Th·ªßy", 2: "Th·ªï", 3: "M·ªôc", 4: "M·ªôc", 6: "Kim", 7: "Kim", 8: "Th·ªï", 9: "H·ªèa"}
