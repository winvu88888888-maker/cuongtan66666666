"""
Enhanced Gemini Helper with Context Awareness - V1.7.3
Gemini s·∫Ω t·ª± ƒë·ªông bi·∫øt ng·ªØ c·∫£nh: cung n√†o, ch·ªß ƒë·ªÅ g√¨, ƒëang xem ph·∫ßn n√†o
"""

import google.generativeai as genai
import os
import requests
import json
import time
import hashlib

# --- DATA CONSTANTS ---
CUNG_NGU_HANH = {
    1: "Th·ªßy",
    2: "Th·ªï",
    3: "M·ªôc",
    4: "M·ªôc",
    5: "Th·ªï",
    6: "Kim",
    7: "Kim",
    8: "Th·ªï",
    9: "H·ªèa"
}

CUNG_TEN = {
    1: "Kh·∫£m (Th·ªßy)",
    2: "Kh√¥n (Th·ªï)",
    3: "Ch·∫•n (M·ªôc)",
    4: "T·ªën (M·ªôc)",
    5: "Trung Cung (Th·ªï)",
    6: "C√†n (Kim)",
    7: "ƒêo√†i (Kim)",
    8: "C·∫•n (Th·ªï)",
    9: "Ly (H·ªèa)"
}

QUAI_NAMES = {1: "Kh·∫£m", 2: "Kh√¥n", 3: "Ch·∫•n", 4: "T·ªën", 6: "C√†n", 7: "ƒêo√†i", 8: "C·∫•n", 9: "Ly"}
QUAI_ELEMENTS = {1: "Th·ªßy", 2: "Th·ªï", 3: "M·ªôc", 4: "M·ªôc", 6: "Kim", 7: "Kim", 8: "Th·ªï", 9: "H·ªèa"}


class GeminiQMDGHelperV173:
    """
    Helper class for Gemini AI with QMDG specific knowledge and grounding - V1.7.3
    """
    _response_cache = {}
    _cache_max_size = 100
    
    def __init__(self, api_key):
        """Initialize Gemini with API key and super intelligence features"""
        self.api_key = api_key
        self.version = "V1.7.5"
        genai.configure(api_key=api_key)
        self._failed_models = set() # Track exhausted models
        self.max_retries = 3
        self.base_delay = 2
        self.hub_searcher = None
        self.n8n_webhook_url = None
        
        # Initialize Hub Searcher
        try:
            from ai_modules.hub_searcher import HubSearcher
            self.hub_searcher = HubSearcher()
        except:
            print("‚ö†Ô∏è Hub Searcher could not be initialized.")

        self.model_priority = [
            "gemini-flash-latest",
            "gemini-pro-latest",
            "gemini-2.0-flash",
            "gemini-exp-1206",
        ]
        self.model = self._get_best_model()

        # Context Memory
        self.current_context = {
            "chart_data": None,
            "topic": None,
            "last_action": None,
            "palace": None
        }

    def _get_cache_key(self, prompt):
        """Generate cache key from prompt"""
        return hashlib.md5(prompt.encode('utf-8')).hexdigest()
        
    def _get_cached_response(self, prompt):
        """Get cached response if exists and not expired"""
        key = self._get_cache_key(prompt)
        if key in self._response_cache:
            entry = self._response_cache[key]
            # Expire after 10 minutes
            if time.time() - entry['time'] < 600:
                print("‚ö° Using cached AI response")
                return entry['response']
        return None

    def _cache_response(self, prompt, response):
        """Cache a response"""
        key = self._get_cache_key(prompt)
        # Prune if too big
        if len(self._response_cache) >= self._cache_max_size:
            # Remove oldest
            oldest = min(self._response_cache.items(), key=lambda x: x[1]['time'])
            del self._response_cache[oldest[0]]
            
        self._response_cache[key] = {
            'response': response,
            'time': time.time()
        }

    def set_n8n_url(self, url):
        self.n8n_webhook_url = url

    def _get_best_model(self):
        """Find the best available model for the current API key"""
        # Try finding a working model from priority list
        for model_name in self.model_priority:
            if model_name in self._failed_models:
                continue
            try:
                m = genai.GenerativeModel(model_name)
                # Quick test to see if we have access / quota
                # We skip the network test to speed up startup, relying on lazy error handling during main call
                # m.generate_content("Ping", request_options={"timeout": 5}) 
                print(f"‚úÖ Selected Model: {model_name}")
                return m
            except Exception as e:
                print(f"‚ö†Ô∏è Model {model_name} failed check: {e}")
                
        # Fallback to flash if all else fails
        return genai.GenerativeModel("gemini-1.5-flash")

    def test_connection(self):
        """Quickly test if the API key and model are working"""
        try:
            response = self.model.generate_content("Hello", request_options={"timeout": 10})
            return True, f"K·∫øt n·ªëi th√†nh c√¥ng t·ªõi model {self.model.model_name}!"
        except Exception as e:
            try:
                # DEBUG: List available models
                available = []
                for m in genai.list_models():
                    if 'generateContent' in m.supported_generation_methods:
                        available.append(m.name)
                
                if not available:
                    return False, f"L·ªói: API Key n√†y kh√¥ng th·∫•y model n√†o c·∫£. (Danh s√°ch r·ªóng). Vui l√≤ng t·∫°o Key m·ªõi."
                
                return False, f"L·ªói k·∫øt n·ªëi: {str(e)}. Model kh·∫£ d·ª•ng: {', '.join(available)}"
            except Exception as e2:
                return False, f"L·ªói k·∫øt n·ªëi: {str(e)}. Kh√¥ng th·ªÉ li·ªát k√™ model: {str(e2)}"

    def _fetch_relevant_hub_data(self, query):
        """Fetch the most relevant context from the Sharded Hub."""
        if not self.hub_searcher: return ""
        try:
            results = self.hub_searcher.search(query, max_results=3)
            if not results: return ""
            
            context_str = "\n\n**KI·∫æN TH·ª®C T·ª™ HUB:**\n"
            for res in results:
                context_str += f"- [{res['category']}] {res['title']}: {res['content_snippet'][:200]}...\n"
            return context_str
        except:
            return ""

    def safe_get_text(self, response):
        """Safely extract text from Gemini response, handling safety blocks"""
        try:
            return response.text
        except ValueError:
            # Handle blocked content
            if response.prompt_feedback:
                if response.prompt_feedback.block_reason:
                    return "‚ö†Ô∏è AI t·ª´ ch·ªëi tr·∫£ l·ªùi v√¨ l√Ω do an to√†n (Safety Filter)."
            return "‚ö†Ô∏è Kh√¥ng c√≥ ph·∫£n h·ªìi vƒÉn b·∫£n (L·ªói kh√¥ng x√°c ƒë·ªãnh)."

    def _call_ai(self, prompt, use_hub=True, use_web_search=False):
        """Call AI with auto-switch fallback, caching, and improved retry logic."""
        
        # Check cache
        cached = self._get_cached_response(prompt)
        if cached: return cached

        # Inject Hub Knowledge
        if use_hub:
            hub_context = self._fetch_relevant_hub_data(prompt[-100:]) # Search based on end of prompt
            if hub_context:
                prompt = hub_context + "\n" + prompt

        # --- N8N HANDOFF (DISABLED FOR DEBUGGING) ---
        # if self.n8n_webhook_url:
        #     try:
        #         print(f"üöÄ Forwarding to n8n: {self.n8n_webhook_url}")
        #         payload = {"prompt": prompt}
        #         resp = requests.post(self.n8n_webhook_url, json=payload, timeout=30)
        #         if resp.status_code == 200:
        #             result = resp.json().get('output', "n8n processed request but returned no output.")
        #             self._cache_response(prompt, result)
        #             return result
        #     except Exception as e:
        #         print(f"‚ö†Ô∏è n8n Error: {e}. Falling back to direct Gemini.")

        # --- DIRECT GEMINI CALL ---
        last_error = ""
        
        for attempt in range(self.max_retries):
            try:
                # Add Web Search capability check (Not native in standard API yet unless using Vertex/special tools)
                # But we can simulate context injection via web_searcher tool if passed in higher level.
                # Here we rely on standard generate_content.
                
                response = self.model.generate_content(prompt)
                
                # Success
                text = self.safe_get_text(response)
                
                # Prepend debug tag
                text = "**üîÆ [PYTHON V1.7.5 DIRECT]**\n\n" + text
                
                self._cache_response(prompt, text)
                return text

            except Exception as e:
                error_msg = str(e)
                last_error = error_msg
                print(f"‚ö†Ô∏è AI Call Error (Attempt {attempt+1}): {error_msg}")
                
                # Handle Quota -> Switch Model
                if "429" in error_msg or "quota" in error_msg.lower():
                    self._failed_models.add(self.model.model_name)
                    print(f"‚ö†Ô∏è Model {self.model.model_name} exhausted. Switching...")
                    self.model = self._get_best_model()
                    time.sleep(2)
                    continue
                
                time.sleep(2) # Retry delay
                continue
                    
        return f"‚ùå L·ªói AI sau {self.max_retries} l·∫ßn th·ª≠: {last_error}"

    def update_context(self, **kwargs):
        """Update current context"""
        self.current_context.update(kwargs)
    
    def get_system_knowledge(self):
        """Returns string representation of key system rules for AI context"""
        return """
**QUY T·∫ÆC LU·∫¨N GI·∫¢I CHUY√äN S√ÇU (V1.7.3):**
1. **Nguy√™n l√Ω Sinh Kh·∫Øc:** Th·ªßy(1)->M·ªôc(3,4)->H·ªèa(9)->Th·ªï(2,8,5)->Kim(6,7)->Th·ªßy(1).
2. **D·ª•ng Th·∫ßn:** Y·∫øu t·ªë ƒë·∫°i di·ªán cho s·ª± vi·ªác.
3. **B·∫£n M·ªánh:** ƒê·∫°i di·ªán cho ng∆∞·ªùi h·ªèi (Can Ng√†y).
4. **K·∫øt Lu·∫≠n:** D·ª±a v√†o sinh kh·∫Øc gi·ªØa Cung B·∫£n M·ªánh v√† Cung D·ª•ng Th·∫ßn.
"""

    def get_context_prompt(self):
        """Build context prompt from current state"""
        context_parts = []
        context_parts.append(self.get_system_knowledge())
        
        if self.current_context.get('topic'):
            context_parts.append(f"**Ch·ªß ƒë·ªÅ hi·ªán t·∫°i:** {self.current_context['topic']}")
        
        if context_parts:
            return "\n".join(["**NG·ªÆ C·∫¢NH H·ªÜ TH·ªêNG:**"] + context_parts) + "\n\n"
        return ""
    
    def classify_topic_intent(self, topic):
        """Classify topic"""
        topic_lower = topic.lower()
        if any(kw in topic_lower for kw in ['ƒë√°nh', 'c√° c∆∞·ª£c', 'x·ªï s·ªë', 'c·ªù b·∫°c']): return 'GAMBLING'
        if any(kw in topic_lower for kw in ['s·ª©c kh·ªèe', 'b·ªánh', 'ch·ªØa']): return 'HEALTH'
        if any(kw in topic_lower for kw in ['kinh doanh', 'ƒë·∫ßu t∆∞', 'l·ª£i nhu·∫≠n']): return 'BUSINESS'
        if any(kw in topic_lower for kw in ['t√¨nh', 'y√™u', 'h√¥n nh√¢n']): return 'RELATIONSHIP'
        return 'GENERAL'
    
    def search_knowledge_hub(self, query, category=None, max_results=3):
        """Search knowledge hub"""
        if not self.hub_searcher: return []
        try: return self.hub_searcher.search(query, category=category, max_results=max_results)
        except: return []

    # --- MAIN Q&A METHOD (STRICTLY FIXED FOR V1.7.3) ---
    def answer_question(self, question, chart_data=None, topic=None):
        """
        Answer with FULL CONTEXT AWARENESS & NO LISTING
        """
        if chart_data is None: chart_data = self.current_context.get('chart_data')
        if topic is None: topic = self.current_context.get('topic', 'Chung')
        
        self.update_context(topic=topic, chart_data=chart_data, last_action=f"[V1.7.3] {question[:50]}...")
        
        context = self.get_context_prompt()
        chart_context = ""
        deep_knowledge = ""
        
        if chart_data:
            # 1. Identify Key Actors
            day_stem = (chart_data.get('can_ngay', '') or '').split(' ')[0]
            hour_stem = (chart_data.get('can_gio', '') or '').split(' ')[0]
            
            day_palace = None
            hour_palace = None
            
            for i in range(1, 10):
                stem_heaven = chart_data.get('can_thien_ban', {}).get(i)
                if stem_heaven == day_stem: day_palace = i
                if stem_heaven == hour_stem: hour_palace = i
            
            # 2. Build Focused Description
            search_queries = []
            context_items = []
            
            def get_palace_desc(p_idx, label):
                if not p_idx: return ""
                sao = chart_data.get('thien_ban', {}).get(p_idx, 'N/A')
                mon = chart_data.get('nhan_ban', {}).get(p_idx, 'N/A')
                than = chart_data.get('than_ban', {}).get(p_idx, 'N/A')
                can_thien = chart_data.get('can_thien_ban', {}).get(p_idx, 'N/A')
                can_dia = chart_data.get('dia_can', {}).get(p_idx, 'N/A')
                
                search_queries.append(f"√ù nghƒ©a sao {sao} c·ª≠a {mon} trong k·ª≥ m√¥n ƒë·ªôn gi√°p")
                return (f"- **{label} (Cung {p_idx})**: "
                        f"G·∫∑p sao **{sao}**, c·ª≠a **{mon}**, th·∫ßn **{than}**. "
                        f"Thi√™n b√†n **{can_thien}** tr√™n ƒë·ªãa b√†n **{can_dia}**.")

            if day_palace:
                context_items.append(get_palace_desc(day_palace, f"B·∫¢N M·ªÜNH NG∆Ø·ªúI H·ªéI (Can Ng√†y {day_stem})"))
            if hour_palace and hour_palace != day_palace:
                context_items.append(get_palace_desc(hour_palace, f"V·∫§N ƒê·ªÄ C·∫¶N H·ªéI (Can Gi·ªù {hour_stem})"))

            # 3. Web Search
            if search_queries:
                try:
                    from ai_modules.web_searcher import get_web_searcher
                    searcher = get_web_searcher()
                    dk_results = []
                    for q in search_queries[:2]:
                        res = searcher.search_google(q, num_results=2)
                        for r in res: dk_results.append(f"- {r.get('title')}: {r.get('snippet')[:100]}")
                    if dk_results:
                        deep_knowledge = "\n**KI·∫æN TH·ª®C B·ªî TR·ª¢ (GOOGLE):**\n" + "\n".join(dk_results)
                except: pass
                
            chart_context = "\n**D·ªÆ LI·ªÜU C·ªêT L√ïI (CH·ªà X√âT B·∫¢N M·ªÜNH & S·ª∞ VI·ªÜC):**\n" + "\n".join(context_items)

        prompt = f"""{context}B·∫°n l√† ƒê·∫†I PH√ÅP S∆Ø K·ª≤ M√îN (V1.7.3) - Ng∆∞·ªùi nh√¨n th·∫•u thi√™n c∆°.

**B·ªêI C·∫¢NH:**
- Ch·ªß ƒë·ªÅ: "{topic}"
{chart_context}
{deep_knowledge}

**C√ÇU H·ªéI:** "{question}"

**NHI·ªÜM V·ª§:**
1. **Lu·∫≠n Gi·∫£i**: D·ª±a v√†o Sao/M√¥n ·ªü Cung B·∫£n M·ªánh v√† Cung S·ª± Vi·ªác tr√™n, gi·∫£i th√≠ch t·∫°i sao t·ªët/x·∫•u?
2. **K·∫øt Lu·∫≠n**: ƒêi th·∫≥ng v√†o k·∫øt qu·∫£ (Th√†nh hay B·∫°i, C√°t hay Hung).
3. **L·ªùi Khuy√™n**: M·ªôt h√†nh ƒë·ªông c·ª• th·ªÉ ƒë·ªÉ c·∫£i m·ªánh.

**TUY·ªÜT ƒê·ªêI KH√îNG LI·ªÜT K√ä C√ÅC CUNG KH√ÅC.** Ch·ªâ t·∫≠p trung v√†o B·∫£n M·ªánh v√† S·ª± Vi·ªác.
Tr·∫£ l·ªùi s·∫Øc s·∫£o, ng·∫Øn g·ªçn, phong c√°ch huy·ªÅn b√≠ nh∆∞ng th·ª±c t·∫ø.
"""
        return self._call_ai(prompt, use_hub=True, use_web_search=True)

    def explain_element(self, element_type, element_name):
        """Explain element"""
        prompt = f"Gi·∫£i th√≠ch ng·∫Øn g·ªçn 3 d√≤ng v·ªÅ √Ω nghƒ©a c·ªßa {element_type} {element_name} trong K·ª≥ M√¥n ƒê·ªôn Gi√°p. T·∫≠p trung v√†o C√°t/Hung."
        return self._call_ai(prompt)

    # ... Include other analysis methods if needed, simplified for brevity but functional ...
    # Integrating core expert logic for full analysis view
    def comprehensive_analysis(self, chart_data, topic, **kwargs):
        return self.answer_question(f"H√£y ph√¢n t√≠ch t·ªïng quan l√° s·ªë n√†y cho ch·ªß ƒë·ªÅ {topic}.", chart_data, topic)

    def analyze_palace(self, palace_data, topic):
        self.answer_question(f"Ph√¢n t√≠ch chi ti·∫øt cung {palace_data.get('num')} cho vi·ªác {topic}.", {'can_ngay': 'X', 'can_thien_ban': {palace_data['num']: 'X'}}, topic) # Dummy data to force palace focus logic if needed, or just use prompt
        # Actually better to use specific prompt
        return self._call_ai(f"Ph√¢n t√≠ch Cung {palace_data} cho ch·ªß ƒë·ªÅ {topic}. ƒê∆∞a ra d·ª± ƒëo√°n ƒë·ªãnh l∆∞·ª£ng % th√†nh c√¥ng.")

    def analyze_mai_hoa(self, mai_hoa_res, topic="Chung"):
        prompt = f"Lu·∫≠n gi·∫£i qu·∫ª Mai Hoa: {mai_hoa_res}. Ch·ªß ƒë·ªÅ: {topic}. K·∫øt lu·∫≠n C√°t/Hung."
        return self._call_ai(prompt)

    def analyze_luc_hao(self, luc_hao_res, topic="Chung"):
        prompt = f"Lu·∫≠n gi·∫£i L·ª•c H√†o: {luc_hao_res['ban']['name']} bi·∫øn {luc_hao_res['bien']['name']}. Ch·ªß ƒë·ªÅ: {topic}."
        return self._call_ai(prompt)

# Compatibility Alias
GeminiQMDGHelper = GeminiQMDGHelperV173
