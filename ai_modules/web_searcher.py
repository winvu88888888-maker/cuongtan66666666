"""
Web Search Module for 50 AI Agents
TÃ¬m kiáº¿m vÃ  thu tháº­p dá»¯ liá»‡u tá»« Google/Internet
"""

import requests
from bs4 import BeautifulSoup
import time
import random
from typing import List, Dict

class WebSearcher:
    """Advanced web searcher for AI agents."""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.search_engines = [
            'https://www.google.com/search?q=',
            'https://duckduckgo.com/html/?q='
        ]
    
    def search_google(self, query: str, num_results: int = 5) -> List[Dict]:
        """
        TÃ¬m kiáº¿m trÃªn Google vÃ  tráº£ vá» danh sÃ¡ch URLs + snippets.
        
        Args:
            query: Tá»« khÃ³a tÃ¬m kiáº¿m
            num_results: Sá»‘ káº¿t quáº£ cáº§n láº¥y
            
        Returns:
            List of dicts with 'title', 'url', 'snippet'
        """
        results = []
        
        try:
            # Format query
            search_url = f"https://www.google.com/search?q={requests.utils.quote(query)}&num={num_results}"
            
            # Add random delay to avoid rate limiting
            time.sleep(random.uniform(1, 3))
            
            response = requests.get(search_url, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Parse search results
                for g in soup.find_all('div', class_='g')[:num_results]:
                    try:
                        title_elem = g.find('h3')
                        link_elem = g.find('a')
                        snippet_elem = g.find('div', class_='VwiC3b')
                        
                        if title_elem and link_elem:
                            title = title_elem.get_text()
                            url = link_elem.get('href', '')
                            snippet = snippet_elem.get_text() if snippet_elem else ""
                            
                            if url.startswith('http'):
                                results.append({
                                    'title': title,
                                    'url': url,
                                    'snippet': snippet
                                })
                    except Exception as e:
                        continue
                        
        except Exception as e:
            print(f"âš ï¸ Search error: {e}")
            
        return results
    
    def extract_content(self, url: str, max_length: int = 5000) -> str:
        """
        TrÃ­ch xuáº¥t ná»™i dung vÄƒn báº£n tá»« URL.
        
        Args:
            url: URL cáº§n crawl
            max_length: Äá»™ dÃ i tá»‘i Ä‘a cá»§a ná»™i dung
            
        Returns:
            Cleaned text content
        """
        try:
            time.sleep(random.uniform(0.5, 2))
            
            response = requests.get(url, headers=self.headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Remove script and style elements
                for script in soup(["script", "style", "nav", "footer", "header"]):
                    script.decompose()
                
                # Get text
                text = soup.get_text()
                
                # Clean up
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                # Truncate if too long
                if len(text) > max_length:
                    text = text[:max_length] + "..."
                
                return text
                
        except Exception as e:
            print(f"âš ï¸ Extract error for {url}: {e}")
            return ""
    
    def deep_research(self, topic: str, num_sources: int = 3) -> str:
        """
        NghiÃªn cá»©u sÃ¢u má»™t chá»§ Ä‘á» tá»« nhiá»u nguá»“n.
        
        Args:
            topic: Chá»§ Ä‘á» cáº§n nghiÃªn cá»©u
            num_sources: Sá»‘ nguá»“n cáº§n crawl
            
        Returns:
            Synthesized content from multiple sources
        """
        print(f"ğŸ” Äang nghiÃªn cá»©u: {topic}")
        
        # Search
        search_results = self.search_google(topic, num_results=num_sources)
        
        if not search_results:
            return f"KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ cho: {topic}"
        
        # Collect content from sources
        collected_data = []
        collected_data.append(f"# NghiÃªn cá»©u chuyÃªn sÃ¢u: {topic}\n\n")
        
        for idx, result in enumerate(search_results, 1):
            collected_data.append(f"## Nguá»“n {idx}: {result['title']}\n")
            collected_data.append(f"**URL**: {result['url']}\n\n")
            
            if result['snippet']:
                collected_data.append(f"**TÃ³m táº¯t**: {result['snippet']}\n\n")
            
            # Extract full content
            content = self.extract_content(result['url'], max_length=2000)
            if content:
                collected_data.append(f"**Ná»™i dung chi tiáº¿t**:\n{content}\n\n")
                collected_data.append("---\n\n")
        
        return ''.join(collected_data)
    
    def quick_search(self, query: str) -> str:
        """
        TÃ¬m kiáº¿m nhanh vÃ  tráº£ vá» tÃ³m táº¯t.
        
        Args:
            query: CÃ¢u há»i/tá»« khÃ³a
            
        Returns:
            Quick summary from search results
        """
        results = self.search_google(query, num_results=3)
        
        if not results:
            return "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin."
        
        summary = f"**Káº¿t quáº£ tÃ¬m kiáº¿m: {query}**\n\n"
        
        for idx, r in enumerate(results, 1):
            summary += f"{idx}. **{r['title']}**\n"
            if r['snippet']:
                summary += f"   {r['snippet']}\n"
            summary += f"   ğŸ”— {r['url']}\n\n"
        
        return summary


# Singleton instance
_searcher_instance = None

def get_web_searcher() -> WebSearcher:
    """Get or create web searcher instance."""
    global _searcher_instance
    if _searcher_instance is None:
        _searcher_instance = WebSearcher()
    return _searcher_instance
