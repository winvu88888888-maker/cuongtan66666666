import sys
import os
import time
import random
import concurrent.futures
from datetime import datetime
import json

# Add paths for local import
current_dir = os.path.dirname(os.path.abspath(__file__)) # .../ai_modules
parent_dir = os.path.dirname(current_dir) # .../UPLOAD_TO_STREAMLIT

# Add parent dir to path to find gemini_helper.py
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

if current_dir not in sys.path:
    sys.path.append(current_dir)

try:
    # Try relative imports first for when run as module
    from .shard_manager import add_entry
    from .mining_strategist import MiningStrategist
    from .maintenance_manager import MaintenanceManager
    from .gemini_helper import GeminiQMDGHelper
except (ImportError, ValueError):
    # Fallback to direct imports
    try:
        from shard_manager import add_entry
        from mining_strategist import MiningStrategist
        from maintenance_manager import MaintenanceManager
        from gemini_helper import GeminiQMDGHelper
    except ImportError:
        # Final fallback for Streamlit context
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        from shard_manager import add_entry
        from mining_strategist import MiningStrategist
        from maintenance_manager import MaintenanceManager
        from gemini_helper import GeminiQMDGHelper

import streamlit as st

CONFIG_PATH = os.path.join(os.path.dirname(current_dir), 'data_hub', 'factory_config.json')

def load_config():
    if os.path.exists(CONFIG_PATH):
        try:
            with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except: pass
    return {"autonomous_247": False, "interval_minutes": 30}

def save_config(config):
    try:
        with open(CONFIG_PATH, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2)
    except: pass

def _single_agent_task(agent_id, topic, api_key):
    """C√¥ng vi·ªác c·ªßa m·ªôt Agent ƒë∆°n l·∫ª - UPGRADED v·ªõi Web Search."""
    print(f"ü§ñ [Agent #{agent_id}] ƒêang ti·∫øp nh·∫≠n m·ª•c ti√™u: {topic}...")
    
    try:
        strategist = MiningStrategist()
        ai_helper = GeminiQMDGHelper(api_key)
        
        # PHASE 1: Web Search ƒë·ªÉ thu th·∫≠p d·ªØ li·ªáu th√¥
        try:
            from web_searcher import get_web_searcher
            searcher = get_web_searcher()
            web_data = searcher.deep_research(topic, num_sources=3)
            print(f"‚úÖ [Agent #{agent_id}] ƒê√£ thu th·∫≠p d·ªØ li·ªáu web")
        except Exception as e:
            print(f"‚ö†Ô∏è [Agent #{agent_id}] Web search failed: {e}")
            web_data = ""
        
        # PHASE 2: AI Synthesis v·ªõi d·ªØ li·ªáu web + Gemini Search
        mining_prompt = strategist.synthesize_mining_prompt(topic)
        if web_data:
            mining_prompt = f"{mining_prompt}\n\n**D·ªÆ LI·ªÜU THU TH·∫¨P T·ª™ WEB:**\n{web_data[:3000]}"
        
        raw_content = ai_helper._call_ai(mining_prompt, use_hub=False, use_web_search=True)
        
        # SMART FILTERING: Parse clean title and category from AI response
        clean_title = topic
        standard_category = "Ki·∫øn Th·ª©c"
        final_content = raw_content
        
        try:
            # Look for JSON block
            if "```json" in raw_content:
                parts = raw_content.split("```json")
                if len(parts) > 1:
                    json_str = parts[1].split("```")[0].strip()
                    meta = json.loads(json_str)
                    clean_title = meta.get("clean_title", topic)
                    standard_category = meta.get("standard_category", "Ki·∫øn Th·ª©c")
                    # Remove the JSON block from final content to keep it clean
                    final_content = raw_content.replace(f"```json{json_str}```", "").strip()
        except Exception as e:
            print(f"‚ö†Ô∏è [Agent #{agent_id}] Smart filtering parse failed: {e}")
        
        id = add_entry(
            title=clean_title,
            content=final_content,
            category=standard_category,
            source=f"Agent #{agent_id} (Qu√¢n ƒêo√†n AI)",
            tags=["autonomous", "hyper-depth", f"agent-{agent_id}"]
        )
        
        if id:
            print(f"‚úÖ [Agent #{agent_id}] KHAI TH√ÅC TH√ÄNH C√îNG: {topic}")
            return True
        else:
            print(f"‚ùå [Agent #{agent_id}] Th·∫•t b·∫°i khi l∆∞u: {topic}")
            return False
            
    except Exception as e:
        print(f"‚ö†Ô∏è [Agent #{agent_id}] G·∫∑p s·ª± c·ªë: {e}")
        return False

def run_mining_cycle(api_key, category=None):
    """Executes one full cycle of autonomous mining with THE 50 AI LEGION."""
    if not api_key:
        print("‚ö†Ô∏è Thi·∫øu API Key.")
        return
        
    strategist = MiningStrategist()
    
    # Update stats
    config = load_config()
    config["last_run"] = time.strftime("%Y-%m-%d %H:%M:%S")
    config["total_cycles"] = config.get("total_cycles", 0) + 1
    save_config(config)

    print("\n" + "="*60)
    print(f"üöÄ K√çCH HO·∫†T QU√ÇN ƒêO√ÄN 50 AI - CHU K·ª≤ #{config['total_cycles']}")
    print("="*60)

    # 1. Generate massive queue - UPGRADED TO 50 AGENTS
    queue_size = 50 # REAL 50 agents execution
    initial_queue = strategist.generate_research_queue(category, count=queue_size)
    
    # DEDUPLICATION: Check hub_index to skip already researched topics
    from shard_manager import search_index
    existing_entries = search_index()
    existing_titles = [e['title'].lower() for e in existing_entries]
    
    queue = []
    for t in initial_queue:
        if t.lower() not in existing_titles and len(queue) < queue_size:
            queue.append(t)
    
    if not queue:
        print("‚ú® T·∫•t c·∫£ ch·ªß ƒë·ªÅ hi·ªán t·∫°i ƒë√£ ƒë∆∞·ª£c khai th√°c. ƒêang t·∫°o ch·ªß ƒë·ªÅ ng·∫´u nhi√™n m·ªõi...")
        queue = [f"{t} - Chuy√™n s√¢u Giai ƒëo·∫°n {random.randint(2, 5)}" for t in initial_queue[:10]]

    print(f"üì° Trung t√¢m ch·ªâ huy ƒë√£ ph√¢n ph·ªëi {len(queue)} nhi·ªám v·ª• cho Qu√¢n ƒëo√†n AI...")
    
    # 2. Parallel Execution (Multi-threaded Agents) - SCALED UP
    # Ch·∫°y 20 agents ƒë·ªìng th·ªùi (TƒÉng c∆∞·ªùng hi·ªáu su·∫•t)
    active_agents = min(len(queue), 20)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=active_agents) as executor:
        futures = []
        for i, topic in enumerate(queue):
            # Assign random Agent ID from 1-50
            agent_id = random.randint(1, 50) 
            futures.append(executor.submit(_single_agent_task, agent_id, topic, api_key))
            time.sleep(1) # Stagger start to be nice to API
            
        # Wait for all
        concurrent.futures.wait(futures)

    # 3. AUTONOMOUS CLEANUP (DISABLED BY USER REQUEST)
    # User only wants basic deduplication, no AI-driven cleanup
    # if config["total_cycles"] % 3 == 0:
    #     print("\n" + "-"*40)
    #     print("üßπ K√≠ch ho·∫°t AI D·ªçn D·∫πp (Sanitation Droid)...")
    #     try:
    #         maintenance = MaintenanceManager()
    #         res = maintenance.run_cleanup_cycle()
    #         print(f"‚ú® B√°o c√°o d·ªçn d·∫πp: X√≥a {res.get('removed',0)} tr√πng l·∫∑p, ƒê√≥ng g√≥i {res.get('bagged',0)} items.")
    #     except Exception as e:
    #         print(f"‚ö†Ô∏è L·ªói d·ªçn d·∫πp: {e}")
    # else:
    #     print("\n‚ú® D·ªØ li·ªáu s·∫°ch s·∫Ω. B·ªè qua b∆∞·ªõc d·ªçn d·∫πp chu k·ª≥ n√†y.")
    
    print("\n‚ú® AI Cleanup ƒë√£ b·ªã v√¥ hi·ªáu h√≥a theo y√™u c·∫ßu ng∆∞·ªùi d√πng.")

    # 4. AUTO DEPLOY TO CLOUD (Git Push)
    # T·ª± ƒë·ªông ƒë·ªìng b·ªô d·ªØ li·ªáu l√™n lu·ªìng Streamlit Cloud ƒë·ªÉ web c·∫≠p nh·∫≠t
    print("\n" + "-"*40)
    print("‚òÅÔ∏è ƒêang ƒë·ªìng b·ªô d·ªØ li·ªáu l√™n ƒê√°m M√¢y (Auto-Deploy)...")
    try:
        import subprocess
        # G·ªçi script sync_and_push.bat ·ªü th∆∞ m·ª•c cha
        sync_script = os.path.join(parent_dir, "sync_and_push.bat")
        if os.path.exists(sync_script):
            subprocess.run([sync_script], shell=True, check=False)
            print("‚úÖ ƒê√£ g·ª≠i l·ªánh ƒë·ªìng b·ªô th√†nh c√¥ng.")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y script ƒë·ªìng b·ªô t·∫°i: {sync_script}")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ƒë·ªìng b·ªô ƒë√°m m√¢y: {e}")

def run_daemon():
    """Persistent 24/7 Loop"""
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë      üè≠ NH√Ä M√ÅY AI V√î T·∫¨N (INFINITE AI FACTORY)      ‚ïë
    ‚ïë           Ch·∫ø ƒë·ªô: 24/7 Autonomous Daemon             ‚ïë
    ‚ïë      T√¨nh tr·∫°ng: üü¢ ƒêANG CH·∫†Y (Background Mode)      ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    error_count = 0
    
    while True:
        # Reload config to check for stop signal (BUT default to TRUE if running from CLI daemon)
        config = load_config()
        
        # Logic: If config says False, we pause but don't exit script completely (wait for enable)
        # unless user kills script.
        
        api_key = config.get("api_key") or os.getenv("GEMINI_API_KEY")
        
        # 1. Truy t√¨m Key trong secrets.toml c·ªßa Streamlit
        if not api_key:
            try:
                secrets_path = os.path.join(os.path.dirname(current_dir), ".streamlit", "secrets.toml")
                if os.path.exists(secrets_path):
                    with open(secrets_path, "r", encoding="utf-8") as f:
                        for line in f:
                            if "GEMINI_API_KEY" in line or "gemini_key" in line:
                                parts = line.split("=")
                                if len(parts) >= 2:
                                    found_key = parts[1].strip().strip('"').strip("'")
                                    if found_key:
                                        api_key = found_key
                                        print(f"‚úÖ ƒê√£ t√¨m th·∫•y API Key t·ª´ secrets.toml")
                                        # Save to factory config for future
                                        config["api_key"] = api_key
                                        save_config(config)
                                        break
            except: pass

        # 2. Truy t√¨m Key trong custom_data.json (Do app.py l∆∞u)
        if not api_key:
            try:
                custom_data_path = os.path.join(os.path.dirname(current_dir), "custom_data.json")
                if os.path.exists(custom_data_path):
                    with open(custom_data_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        found_key = data.get("GEMINI_API_KEY")
                        if found_key:
                            api_key = found_key
                            print(f"‚úÖ ƒê√£ t√¨m th·∫•y API Key t·ª´ custom_data.json")
                            # AUTO-SYNC: L∆∞u v√†o factory config ƒë·ªÉ d√πng cho c√°c l·∫ßn sau
                            config["api_key"] = api_key
                            save_config(config)
                            print(f"üîÑ ƒê√£ ƒë·ªìng b·ªô API Key v√†o factory_config.json")
            except: pass

        # 3. N·∫øu v·∫´n ch∆∞a c√≥, h·ªèi ng∆∞·ªùi d√πng NH·∫¨P TR·ª∞C TI·∫æP
        if not api_key:
            print("\n‚ùå CH∆ØA T√åM TH·∫§Y API KEY.")
            print("üëâ Vui l√≤ng nh·∫≠p Gemini API Key c·ªßa b·∫°n v√†o b√™n d∆∞·ªõi ƒë·ªÉ c·∫•u h√¨nh m·ªôt l·∫ßn duy nh·∫•t.")
            print("(Key s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o factory_config.json v√† custom_data.json ƒë·ªÉ t·ª± ƒë·ªông ch·∫°y c√°c l·∫ßn sau)")
            try:
                user_input_key = input("üîë Nh·∫≠p API Key: ").strip()
                if user_input_key and len(user_input_key) > 10:
                    api_key = user_input_key
                    config["api_key"] = api_key
                    save_config(config)
                    
                    # C·ª∞C K·ª≤ QUAN TR·ªåNG: L∆∞u ƒë·ªìng th·ªùi v√†o custom_data.json cho app.py th·∫•y
                    try:
                        custom_data_path = os.path.join(os.path.dirname(current_dir), "custom_data.json")
                        c_data = {}
                        if os.path.exists(custom_data_path):
                            with open(custom_data_path, 'r', encoding='utf-8') as f:
                                c_data = json.load(f)
                        c_data["GEMINI_API_KEY"] = api_key
                        with open(custom_data_path, 'w', encoding='utf-8') as f:
                            json.dump(c_data, f, ensure_ascii=False, indent=4)
                        print(f"‚úÖ ƒê√£ l∆∞u c·∫•u h√¨nh v√† ƒë·ªìng b·ªô sang custom_data.json")
                    except: pass
                    
                    print("‚úÖ Kh·ªüi ƒë·ªông th√†nh c√¥ng!")
                else:
                    print("‚ö†Ô∏è Key kh√¥ng h·ª£p l·ªá. ƒêang th·ª≠ l·∫°i sau 60s...")
                    time.sleep(60)
                    continue
            except Exception:
                # Tr∆∞·ªùng h·ª£p kh√¥ng input ƒë∆∞·ª£c (v√≠ d·ª• ch·∫°y ng·∫ßm ho√†n to√†n kh√¥ng t∆∞∆°ng t√°c)
                time.sleep(60)
                continue
            
        # Run Cycle
        try:
            run_mining_cycle(api_key)
            error_count = 0 # Reset error count on success
        except Exception as e:
            error_count += 1
            print(f"üî• L·ªói h·ªá th·ªëng: {e}")
            if error_count > 10:
                print("‚ö†Ô∏è Qu√° nhi·ªÅu l·ªói li√™n ti·∫øp. T·∫°m d·ª´ng 10 ph√∫t...")
                time.sleep(600)
                error_count = 0
            
        # Sleep
        interval = config.get("interval_minutes", 15) # Default faster (15 mins)
        if interval < 1: interval = 1
        
        next_run = time.time() + (interval * 60)
        print(f"\n‚è≥ Qu√¢n ƒëo√†n AI ngh·ªâ ng∆°i {interval} ph√∫t...")
        print(f"‚è∞ Chu k·ª≥ ti·∫øp theo: {datetime.fromtimestamp(next_run).strftime('%H:%M:%S')}")
        
        # Countdown visual (optional)
        time.sleep(interval * 60)

if __name__ == "__main__":
    # Check if running in GitHub Actions
    is_github_action = os.getenv("GITHUB_ACTIONS") == "true"
    
    if "--daemon" in sys.argv and not is_github_action:
        # Force enable in config if running explicitly
        c = load_config()
        c["autonomous_247"] = True
        save_config(c)
        run_daemon()
    else:
        # One-off run (Local or GitHub Action)
        print("üöÄ Ch·∫°y ch·∫ø ƒë·ªô One-Off (Khai th√°c 1 l·∫ßn r·ªìi ngh·ªâ)...")
        
        # Priority: Env Var (GitHub Secrets) -> Config -> Custom Data
        key = os.environ.get("GEMINI_API_KEY")
        
        if not key:
            # Try load from config
            c = load_config()
            key = c.get("api_key")
            
        if not key:
             # Try custom_data.json
             try:
                custom_data_path = os.path.join(os.path.dirname(current_dir), "custom_data.json")
                if os.path.exists(custom_data_path):
                    with open(custom_data_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        key = data.get("GEMINI_API_KEY")
             except: pass

        if key:
            run_mining_cycle(key)
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y Key. (N·∫øu ch·∫°y tr√™n GitHub, h√£y set Secret GEMINI_API_KEY)")
            # Tr√™n GitHub Actions, kh√¥ng error exit ƒë·ªÉ tr√°nh b√°o ƒë·ªè c·∫£ workflow n·∫øu ch·ªâ thi·∫øu key
            if is_github_action:
                print("‚ö†Ô∏è B·ªè qua chu k·ª≥ n√†y do thi·∫øu Key.")
            else:
                sys.exit(1)
