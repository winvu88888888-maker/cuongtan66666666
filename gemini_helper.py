"""
Enhanced Gemini Helper with Context Awareness (V2.2 - Smart Router)
"""

import google.generativeai as genai
import os
import requests
import json
import time
import hashlib
import re

# Import QMDG Complete Knowledge Base
try:
    from qmdg_knowledge_complete import (
        CUU_CUNG, CUU_TINH, BAT_MON, BAT_THAN, THAP_THIEN_CAN,
        tra_cuu_cung, tra_cuu_sao, tra_cuu_mon, tra_cuu_than, tra_cuu_can,
        xac_dinh_gioi_tinh_ke_lay, xac_dinh_huong_khoang_cach, kha_nang_tim_duoc
    )
    QMDG_KNOWLEDGE_LOADED = True
except ImportError:
    QMDG_KNOWLEDGE_LOADED = False

# Import Advanced Rules (64 Qu·∫ª, M√†u s·∫Øc, Quen/L·∫°)
try:
    from qmdg_advanced_rules import (
        MAU_SAC_NGU_HANH, QUEN_LA_QUY_TAC, KHOANG_CACH_CHI_TIET,
        KHA_NANG_LAY_LAI, QUE_64, phan_tich_tim_do_chi_tiet
    )
    ADVANCED_RULES_LOADED = True
except ImportError:
    ADVANCED_RULES_LOADED = False

# Import Complete Inference Rules (M√†u s·∫Øc, Gi√° tr·ªã, K·∫ª tr·ªôm b·ªã b·∫Øt, v.v.)
try:
    from qmdg_inference_rules import (
        phan_tich_toan_dien_tim_do, format_ket_qua_cho_ai,
        tinh_mau_sac_vat, tinh_gia_tri_vat, tinh_khoang_cach,
        tinh_kha_nang_bi_bat, xac_dinh_ke_lay
    )
    INFERENCE_RULES_LOADED = True
except ImportError:
    INFERENCE_RULES_LOADED = False

# Import Auto-Learning System
try:
    from auto_knowledge_updater import (
        auto_learn_from_question, get_learned_knowledge,
        LINH_VUC_CO_SAN, get_field_detail_level
    )
    AUTO_LEARN_LOADED = True
except ImportError:
    AUTO_LEARN_LOADED = False

# Robust Fallback Import
try:
    from free_ai_helper import FreeAIHelper
except ImportError:
    class FreeAIHelper:
        def __getattr__(self, name):
            return lambda *args, **kwargs: "‚ö†Ô∏è Ch·∫ø ƒë·ªô Offline kh√¥ng kh·∫£ d·ª•ng (L·ªói Import)."

class GeminiQMDGHelper:
    """Helper class for Gemini AI with QMDG specific knowledge and grounding"""
    
    _response_cache = {}
    _cache_max_size = 100
    
    def __init__(self, api_key_input):
        # ROBUST KEY EXTRACTION
        self.api_keys = re.findall(r"AIza[0-9A-Za-z-_]{35}", str(api_key_input))
        if not self.api_keys and api_key_input:
             self.api_keys = [k.strip() for k in str(api_key_input).split(',') if k.strip()]

        self.current_key_index = 0
        self.api_key = self.api_keys[0] if self.api_keys else None
        
        self.version = "V2.2-SmartRouter" # Marked to verify update
        if self.api_key:
            genai.configure(api_key=self.api_key)
        
        self._failed_models = set()
        self._hashlib = hashlib
        self.max_retries = 2
        self.base_delay = 1
        self.n8n_url = None
        self.n8n_timeout = 8
        
        self.model = self._get_best_model()
        self.fallback_helper = FreeAIHelper()

    def _get_best_model(self):
        # Default placeholder, actual model is found in test_connection
        return genai.GenerativeModel('gemini-1.5-flash')

    def test_connection(self):
        try:
            # 1. Ask Google: "What models do I have?"
            # This is the most robust way to avoid 404s on names.
            valid_models = []
            try:
                available = list(genai.list_models())
                for m in available:
                    if 'generateContent' in m.supported_generation_methods:
                        valid_models.append(m.name)
            except Exception as e:
                return False, f"L·ªói li·ªát k√™ model (API Key h·ªèng?): {str(e)}"

            if not valid_models:
                return False, "Key n√†y kh√¥ng c√≥ quy·ªÅn truy c·∫≠p b·∫•t k·ª≥ Model n√†o!"

            # 2. Prioritize modern models
            # We sort/filter to pick the best one.
            # Names come like 'models/gemini-1.5-flash-001'
            priority_order = ['gemini-2.0-flash', 'gemini-2.5-flash', 'gemini-2.0-flash-001']
            
            chosen_model_name = None
            
            # Simple match logic
            for p in priority_order:
                for vm in valid_models:
                    if p in vm:
                        chosen_model_name = vm
                        break
                if chosen_model_name: break
            
            # Fallback to FIRST valid model if no priority match
            if not chosen_model_name:
                chosen_model_name = valid_models[0]

            # 3. Final Test
            self.model = genai.GenerativeModel(chosen_model_name)
            self.model.generate_content("ping")
            self.active_model_name = chosen_model_name
            
            return True, f"K·∫øt n·ªëi OK! (Model: {chosen_model_name})"

        except Exception as e:
            return False, f"L·ªói k·∫øt n·ªëi cu·ªëi c√πng: {str(e)}"


            
    def set_n8n_url(self, url):
        self.n8n_url = url

    # --- CORE INTELLIGENCE: INTENT CLASSIFIER ---
    def classify_intent(self, text):
        """Ph√¢n lo·∫°i √Ω ƒë·ªãnh: 'social' vs 'question'"""
        text = text.lower().strip()
        social_keywords = ["ch√†o", "hello", "hi", "b·∫°n ∆°i", "alo", "c√≥ ƒë√≥ kh√¥ng", "gi·ªèi qu√°", "hay qu√°", "t·∫°m bi·ªát", "c·∫£m ∆°n"]
        if len(text.split()) < 5 and any(k in text for k in social_keywords): return 'social'
        return 'question'

    def call_n8n_webhook(self, question, context_summary):
        """G·ªçi n8n ƒë·ªÉ l·∫•y d·ªØ li·ªáu th·ª±c t·∫ø"""
        if not self.n8n_url: return None
        try:
            # Standard Schema
            payload = {
                "question": question,
                "context": context_summary,
                "timestamp": str(self._hashlib.sha256(question.encode()).hexdigest())[:10]
            }
            resp = requests.post(self.n8n_url, json=payload, timeout=self.n8n_timeout)
            if resp.status_code == 200:
                data = resp.json()
                # Support multiple schema variations
                return data.get('output') or data.get('text') or data.get('result')
            return None
        except Exception as e:
            print(f"n8n Error: {e}")
            return None

    # --- MASTERMIND: PROMPT ENGINEERING ---
    def _create_expert_prompt(self, user_input):
        import streamlit as st
        
        # 1. Gather State
        try:
            current_topic = st.session_state.get('chu_de_hien_tai', 'Chung')
        except: current_topic = "Chung"

        is_def = any(k in user_input.lower() for k in ["l√† g√¨", "nghƒ©a l√†", "√Ω nghƒ©a"])
        
        # 2. Intent Classification
        intent = self.classify_intent(user_input)
        
        # 3. Knowledge Retrieval
        knowledge = ""
        
        # A. Social
        if intent == 'social':
            knowledge += "[CH·∫æ ƒê·ªò X√É GIAO]: Ng∆∞·ªùi d√πng ch√†o h·ªèi. H√£y ƒë√°p l·∫°i ng·∫Øn g·ªçn, th√¢n thi·ªán, kh√¥ng ph√¢n t√≠ch."
        
        # B. Definition (Dictionary)
        elif is_def:
            try:
                from skill_library import lookup_concept
                defin = lookup_concept(user_input)
                if defin:
                    knowledge += f"\n[T·ª™ ƒêI·ªÇN]: {defin['summary']}\n(Y√äU C·∫¶U: Tr·∫£ l·ªùi ƒë√∫ng ƒë·ªãnh nghƒ©a n√†y.)"
            except: pass
            
        # C. Topic Binding (Only if not definition)
        if intent == 'question' and not is_def:
             knowledge += f"\n[CH·ª¶ ƒê·ªÄ UI]: {current_topic}\n"
             
        # D. n8n
        if intent == 'question' and self.n8n_url:
             n8n_data = self.call_n8n_webhook(user_input, f"Topic: {current_topic}")
             if n8n_data:
                 knowledge += f"\n[D·ªÆ LI·ªÜU TH·ª∞C T·∫æ N8N]: {n8n_data}\n"

        # 4. System Prompt - KI·∫æN TH·ª®C HO√ÄN CH·ªàNH (QMDG + MAI HOA + KINH D·ªäCH)
        sys_prompt = (
            "VAI TR√í: Chuy√™n gia Huy·ªÅn H·ªçc (K·ª≥ M√¥n + Mai Hoa + Kinh D·ªãch)\n"
            "NGUY√äN T·∫ÆC: KH√îNG n√≥i 'kh√¥ng c√≥ th√¥ng tin' - LU√îN suy lu·∫≠n v√† ƒë∆∞a S·ªê LI·ªÜU C·ª§ TH·ªÇ\n\n"
            
            "=== üîÆ B·∫¢NG 8 QU√ÅI (MAI HOA + KINH D·ªäCH) ===\n"
            "| Qu√°i | H√†nh | H∆∞·ªõng | Gi·ªõi t√≠nh | Tu·ªïi | M√†u s·∫Øc |\n"
            "| C√†n | Kim | T√ÇY B·∫ÆC | NAM, cha, l√£nh ƒë·∫°o | >50 | Tr·∫Øng |\n"
            "| Kh√¥n | Th·ªï | T√ÇY NAM | N·ªÆ, m·∫π, ph·ª• n·ªØ gi√† | >45 | V√†ng/N√¢u |\n"
            "| Ch·∫•n | M·ªôc | ƒê√îNG | NAM, con trai c·∫£ | 25-40 | Xanh l√° |\n"
            "| T·ªën | M·ªôc | ƒê√îNG NAM | N·ªÆ, con g√°i c·∫£ | 25-35 | Xanh l√° |\n"
            "| Kh·∫£m | Th·ªßy | B·∫ÆC | NAM, con trai gi·ªØa | 30-45 | ƒêen/Xanh d∆∞∆°ng |\n"
            "| Ly | H·ªèa | NAM | N·ªÆ, con g√°i gi·ªØa | 25-40 | ƒê·ªè/Cam |\n"
            "| C·∫•n | Th·ªï | ƒê√îNG B·∫ÆC | NAM, con trai √∫t | 15-25 | V√†ng/N√¢u |\n"
            "| ƒêo√†i | Kim | T√ÇY | N·ªÆ, con g√°i √∫t | 15-25 | Tr·∫Øng |\n\n"
            
            "=== üìç B·∫¢NG CUNG V·ªä (KHO·∫¢NG C√ÅCH + H∆Ø·ªöNG) ===\n"
            "| Cung | Qu√°i | H∆∞·ªõng | Kho·∫£ng c√°ch | ƒê·ªãa ƒëi·ªÉm |\n"
            "| 1 | Kh·∫£m | B·∫ÆC | 100-1000m | N∆°i c√≥ n∆∞·ªõc, WC |\n"
            "| 2 | Kh√¥n | T√ÇY NAM | 50-500m | ƒê·∫•t tr·ªëng, ru·ªông |\n"
            "| 3 | Ch·∫•n | ƒê√îNG | 300-3000m | Ch·ª£, n∆°i ƒë√¥ng ng∆∞·ªùi |\n"
            "| 4 | T·ªën | ƒê√îNG NAM | 400-4000m | VƒÉn ph√≤ng, n∆°i cao |\n"
            "| 5 | - | T·∫†I CH·ªñ | 0-200m | Trong nh√† |\n"
            "| 6 | C√†n | T√ÇY B·∫ÆC | 600-6000m | C∆° quan, nh√† cao |\n"
            "| 7 | ƒêo√†i | T√ÇY | 70-700m | Qu√°n x√°, karaoke |\n"
            "| 8 | C·∫•n | ƒê√îNG B·∫ÆC | 80-800m | N√∫i, kho, c·ª≠a h√†ng |\n"
            "| 9 | Ly | NAM | 900-9000m | Tr∆∞·ªùng h·ªçc, nh√† b·∫øp |\n\n"
            
            "=== üë§ GI·ªöI T√çNH NG∆Ø·ªúI L·∫§Y ===\n"
            "NAM: Canh, M·∫≠u, Nh√¢m, B√≠nh + Thi√™n B·ªìng, Huy·ªÅn V≈©, B·∫°ch H·ªï + C√†n, Ch·∫•n, Kh·∫£m, C·∫•n\n"
            "N·ªÆ: ·∫§t, K·ª∑, Qu√Ω, ƒêinh + Th√°i √Çm, L·ª•c H·ª£p + Kh√¥n, T·ªën, Ly, ƒêo√†i\n\n"
            
            "=== üé® M√ÄU S·∫ÆC V·∫¨T M·∫§T (THEO NG≈® H√ÄNH) ===\n"
            "Kim = TR·∫ÆNG/B·∫†C | M·ªôc = XANH L√Å | Th·ªßy = ƒêEN | H·ªèa = ƒê·ªé | Th·ªï = V√ÄNG/N√ÇU\n\n"
            
            "=== üîó QUEN HAY L·∫† ===\n"
            "Huy·ªÅn V≈©/B·∫°ch H·ªï = 90% NG∆Ø·ªúI L·∫† | Th√°i √Çm/L·ª•c H·ª£p = 70% QUEN\n"
            "Cung 2/5/8 = 70% QUEN | Cung 1/6 = 70% L·∫†\n\n"
            
            "=== üìä KH·∫¢ NƒÇNG T√åM ƒê∆Ø·ª¢C (THEO M√îN) ===\n"
            "Sinh=85% | H∆∞u=80% | Khai=70% | C·∫£nh=50% | ƒê·ªó=40% | Th∆∞∆°ng=25% | Kinh=15% | T·ª≠=5%\n\n"
            
            "=== ‚úÖ FORMAT TR·∫¢ L·ªúI B·∫ÆT BU·ªòC ===\n"
            "üë§ Ai l·∫•y: [NAM/N·ªÆ], [XX-XX] tu·ªïi (CƒÉn c·ª©: [Can/Th·∫ßn/Qu√°i])\n"
            "üìç H∆∞·ªõng: [H∆Ø·ªöNG] (CƒÉn c·ª©: Cung [X] = Qu√°i [X])\n"
            "üìè Kho·∫£ng c√°ch: [XXX-XXXm] (CƒÉn c·ª©: Cung [X])\n"
            "üé® M√†u s·∫Øc: [M√ÄU] (CƒÉn c·ª©: H√†nh [X])\n"
            "üîó Quen/L·∫°: [X%] (CƒÉn c·ª©: [Th·∫ßn])\n"
            "üîÑ L·∫•y l·∫°i ƒë∆∞·ª£c: [X%] (CƒÉn c·ª©: [M√¥n])\n\n"
            
            f"TH√îNG TIN B·ªî SUNG: {knowledge}\n"
        )
        return sys_prompt + f"\nUSER: {user_input}"

    def safe_get_text(self, response):
        try:
            if not response.candidates: return "‚ö†Ô∏è"
            if response.text: return response.text
        except: pass
        return "‚ö†Ô∏è"

    # --- BASIC AI CALLER WITH RETRY + FALLBACK + DEBUG LOGGING ---
    def _call_ai_raw(self, prompt):
        import random
        from datetime import datetime
        
        # DEBUG: Log start
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"\n{'='*50}")
        print(f"[STEP 1] üöÄ _call_ai_raw START @ {timestamp}")
        print(f"[STEP 1] üìù Prompt length: {len(str(prompt))} chars")
        
        # CASCADE: CURRENT MODELS (Feb 2026) - FROM GOOGLE DOCS!
        # ‚ö†Ô∏è Gemini 1.5 is DEPRECATED - removed!
        cascade_models = [
            'gemini-2.0-flash',              # Free tier, fast
            'gemini-2.0-flash-001',          # Stable version
            'gemini-2.5-flash',              # Latest stable (may need paid)
            'gemini-2.5-flash-lite',         # Low cost alternative
        ]
        print(f"[STEP 2] üìã Models to try: {cascade_models}")
        
        max_retries = 3
        base_delay = 1
        last_error = None
        error_log = []
        
        for model_idx, model_name in enumerate(cascade_models):
            print(f"\n[STEP 3.{model_idx+1}] üîÑ Trying model: {model_name}")
            
            for attempt in range(max_retries):
                try:
                    print(f"[STEP 4] ‚è≥ Attempt {attempt+1}/{max_retries} with {model_name}")
                    
                    import google.generativeai as genai
                    active_model = genai.GenerativeModel(model_name)
                    print(f"[STEP 5] ‚úÖ Model instantiated: {model_name}")
                    
                    # Try with search tools first
                    try:
                        print(f"[STEP 6a] üîç Trying with google_search_retrieval...")
                        tools = [{"google_search_retrieval": {}}]
                        resp = active_model.generate_content(prompt, tools=tools)
                        print(f"[STEP 6a] ‚úÖ Response with tools OK")
                    except Exception as tool_err:
                        print(f"[STEP 6b] ‚ö†Ô∏è Tools failed: {str(tool_err)[:30]}. Trying without...")
                        resp = active_model.generate_content(prompt)
                        print(f"[STEP 6b] ‚úÖ Response without tools OK")
                    
                    if resp.text: 
                        print(f"[STEP 7] üéâ SUCCESS! Response length: {len(resp.text)} chars")
                        print(f"{'='*50}\n")
                        return resp.text
                    else:
                        print(f"[STEP 7] ‚ö†Ô∏è Empty response from {model_name}")
                        
                except Exception as e:
                    err = str(e).lower()
                    err_short = err[:80]
                    error_log.append(f"{model_name}@attempt{attempt+1}: {err_short}")
                    print(f"[STEP ERROR] ‚ùå {model_name} failed: {err_short}")
                    
                    # LEAKED KEY - Stop immediately
                    if "leaked" in err or "403" in err or ("key" in err and "invalid" in err):
                        print(f"[STEP ERROR] üõë API KEY BLOCKED! Stopping all retries.")
                        return "üõë L·ªñI API KEY: Key ƒë√£ b·ªã kh√≥a. Vui l√≤ng ƒë·ªïi Key m·ªõi!"
                    
                    # 404 - Model not found
                    if "404" in err or "not found" in err:
                        print(f"[STEP ERROR] üîç Model {model_name} NOT FOUND. Trying next...")
                        break  # Skip to next model
                    
                    # QUOTA - Retry with backoff
                    if "429" in err or "quota" in err:
                        if attempt < max_retries - 1:
                            delay = base_delay * (2 ** attempt) + random.uniform(0, 0.5)
                            print(f"[STEP RETRY] ‚è≥ Rate limited. Waiting {delay:.1f}s...")
                            time.sleep(delay)
                            continue
                        else:
                            print(f"[STEP RETRY] ‚ö†Ô∏è {model_name} exhausted after {max_retries} retries")
                            last_error = e
                            break
                    
                    print(f"[STEP ERROR] ‚ö†Ô∏è Unknown error. Trying next model...")
                    last_error = e
                    break
        
        # FALLBACK TO FREE AI
        print(f"\n[STEP 8] üÜò All Gemini models exhausted. Activating FREE AI backup...")
        try:
            if self.fallback_helper:
                print(f"[STEP 8a] üìû Calling FreeAIHelper...")
                free_response = self.fallback_helper.answer_question(str(prompt)[:500])
                if free_response and len(str(free_response)) > 10:
                    print(f"[STEP 8a] ‚úÖ Free AI responded: {len(str(free_response))} chars")
                    return f"[FREE AI BACKUP] {free_response}"
                else:
                    print(f"[STEP 8a] ‚ö†Ô∏è Free AI returned empty/short response")
            else:
                print(f"[STEP 8a] ‚ùå No fallback_helper available!")
        except Exception as fallback_err:
            print(f"[STEP 8b] ‚ùå Free AI failed: {fallback_err}")
        
        # LAST RESORT
        print(f"[STEP 9] üíÄ ALL OPTIONS EXHAUSTED. Returning error message.")
        print(f"[STEP 9] üìã Error log: {error_log}")
        print(f"{'='*50}\n")
        
        return (
            "‚è≥ **AI t·∫°m th·ªùi qu√° t·∫£i**\n\n"
            "**ƒê√£ th·ª≠:**\n"
            f"‚úÖ {len(cascade_models)} models Gemini\n"
            "‚úÖ Free AI Backup\n\n"
            "**Gi·∫£i ph√°p:**\n"
            "1. ‚è∞ ƒê·ª£i 60 gi√¢y r·ªìi th·ª≠ l·∫°i\n"
            "2. üîë D√πng API key kh√°c\n"
            f"\n_Debug Log: {'; '.join(error_log[-3:])}_"
        )
    
    # --- WRAPPED METHODS FOR OFFLINE RESILIENCE ---
    def _call_ai(self, prompt, use_hub=True, use_web_search=False):
        return self._call_ai_raw(prompt)

    # --- PROCESS RESPONSE (Parsing logic) ---
    def _process_response(self, text):
        import re
        import streamlit as st
        
        thinking = ""
        answer = text
        
        # Regex search for the thinking block
        match_thinking = re.search(r'\[SUY_LUAN\](.*?)\[/SUY_LUAN\]', text, re.DOTALL)
        if match_thinking:
            thinking = match_thinking.group(1).strip()
            answer = text.replace(match_thinking.group(0), "").strip()
            
            # Display the thinking process visually (AntiGravity Style)
            st.markdown("""
            <style>
            .ag-thinking {
                background-color: #f0f9ff;
                border: 1px solid #7dd3fc;
                border-radius: 8px;
                padding: 10px;
                font-family: monospace;
                font-size: 0.9em;
                color: #0369a1;
                margin-bottom: 10px;
            }
            </style>
            """, unsafe_allow_html=True)
            with st.expander("‚ö° Antigravity Quy Tr√¨nh T∆∞ Duy (Click ƒë·ªÉ xem)", expanded=False):
                st.markdown(f'<div class="ag-thinking">{thinking}</div>', unsafe_allow_html=True)

        # Regex search for the Conclusion block (New Standard)
        match_conclusion = re.search(r'\[KET_LUAN\](.*?)\[/KET_LUAN\]', answer, re.DOTALL)
        if match_conclusion:
            answer = match_conclusion.group(1).strip()
        
        # Fallback: If AI put everything in thinking block and answer is empty
        if not answer.strip() and thinking:
            answer = "‚ÑπÔ∏è **K·∫øt qu·∫£ t·ª´ quy tr√¨nh suy lu·∫≠n:**\n\n" + thinking
            
        return answer


    def answer_question(self, question, chart_data=None, topic=None): 
        # 1. CLASSIFY INTENT
        import streamlit as st
        
        intent = self.classify_intent(question)
        
        # 2. FAST PATH: SOCIAL & GREETING
        if intent == 'social':
            # Bypass Orchestrator for simple greetings
            return self._call_ai_raw(f"User n√≥i: '{question}'. H√£y ƒë√°p l·∫°i th·∫≠t ng·∫Øn g·ªçn, th√¢n thi·ªán (1 c√¢u). V√≠ d·ª•: 'Ch√†o b·∫°n, t√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?'")

        # 3. KNOWLEDGE PATH: DEFINITIONS
        is_def = any(k in question.lower() for k in ["l√† g√¨", "nghƒ©a l√†", "√Ω nghƒ©a", "gi·∫£i th√≠ch"])
        if is_def:
             prompt = self._create_expert_prompt(question)
             return self._call_ai_raw(prompt)

        # 4. EXTERNAL PATH: N8N (News, Real-time Data)
        n8n_result = None
        # Only call n8n if url is set AND not a pure metaphysical term lookup
        # (This prevents calling n8n for "Sinh M√¥n l√† g√¨")
        if self.n8n_url and not any(k in question.lower() for k in ["b√†n c·ªù", "d·ª•ng th·∫ßn", "cung", "qu·∫ª", "sao", "c·ª≠a"]):
             try:
                n8n_result = self.call_n8n_webhook(question, f"User Interest: {topic}")
             except: pass
        
        # If n8n gave a clear result, use it directly!
        if n8n_result:
             # Synthesize n8n result simply
             prompt = (
                 f"User h·ªèi: {question}\n"
                 f"Th√¥ng tin t√¨m ƒë∆∞·ª£c t·ª´ Internet (N8N): {n8n_result}\n"
                 f"Y√™u c·∫ßu: Tr·∫£ l·ªùi c√¢u h·ªèi user d·ª±a tr√™n th√¥ng tin tr√™n. Ng·∫Øn g·ªçn, s√∫c t√≠ch."
             )
             return self._call_ai_raw(prompt)

        # 5. DEEP PATH: CALCULATOR & ANALYST (Orchestrator)
        from qmdg_orchestrator import AIOrchestrator
        orc = AIOrchestrator(self)
        
        raw = orc.run_pipeline(
            question, 
            current_topic=topic or "Chung", 
            chart_data=chart_data or st.session_state.get('chart_data'),
            mai_hoa_data=st.session_state.get('mai_hoa_result'),
            luc_hao_data=st.session_state.get('luc_hao_result')
        )
        return self._process_response(raw)

    def analyze_palace(self, palace_data, topic): 
        prompt = self._create_expert_prompt(f"Ph√¢n t√≠ch Cung chi ti·∫øt ({topic})")
        # Reuse logic? No, specific analysis needs tools.
        # Ideally this should also use orchestrator for consistency, but for now raw is fine or Orchestrator.
        # Let's keep it using _create_expert which injects context.
        prompt = f"Ph√¢n t√≠ch Cung: {topic}. Data: {json.dumps(palace_data)}"
        return self._call_ai_raw(prompt)

    def explain_element(self, type, name):
         return self.answer_question(f"Gi·∫£i th√≠ch {type} {name}")
    
    def analyze_mai_hao(self, res_data, topic="Chung"): 
        return self.answer_question(f"Lu·∫≠n qu·∫ª Mai Hoa ({topic}): {json.dumps(res_data)}")

    def analyze_luc_hao(self, res_data, topic="Chung"): 
         return self.answer_question(f"Lu·∫≠n qu·∫ª L·ª•c H√†o ({topic}): {json.dumps(res_data)}")

    def comprehensive_analysis(self, chart_data, topic, dung_than_info=None): 
         return self.answer_question(f"T·ªïng quan b√†n K·ª≥ M√¥n ({topic})")
