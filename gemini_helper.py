"""
Enhanced Gemini Helper with Context Awareness
Gemini s·∫Ω t·ª± ƒë·ªông bi·∫øt ng·ªØ c·∫£nh: cung n√†o, ch·ªß ƒë·ªÅ g√¨, ƒëang xem ph·∫ßn n√†o
"""

import google.generativeai as genai
import os
import requests
import json

class GeminiQMDGHelper:
    """Helper class with context awareness for QMDG analysis"""
    
    def __init__(self, api_key):
        """Initialize Gemini with API key"""
        self.api_key = api_key
        genai.configure(api_key=api_key)
        
        # Context tracking - Initialize BEFORE model selection
        self.current_context = {
            'topic': None,
            'palace': None,
            'chart_data': None,
            'last_action': None,
            'dung_than': []
        }
        
        # Adaptive model selection
        self.model = self._get_best_model()

        # n8n endpoint (optional)
        self.n8n_url = None
    
    def set_n8n_url(self, url):
        """Set n8n webhook URL for processing"""
        self.n8n_url = url

    def _get_best_model(self):
        """Find the best available model for the current API key"""
        # Prioritize 1.5 Pro because "gemini t·ªët nh·∫•t"
        models_to_try = [
            'gemini-2.0-flash-exp', # Try latest 2.0 flash
            'gemini-1.5-pro-latest', 
            'gemini-1.5-pro',
            'gemini-1.5-flash-latest', 
            'gemini-1.5-flash',
            'gemini-pro', 
            'gemini-1.0-pro'
        ]
        
        last_error = "Unknown error"
        for model_name in models_to_try:
            try:
                model = genai.GenerativeModel(model_name)
                # Quick test with low tokens
                model.generate_content("ping", generation_config={"max_output_tokens": 1})
                return model
            except Exception as e:
                last_error = str(e)
                continue
        
        # Fallback to list models if configured ones fail
        try:
            for m in genai.list_models():
                if 'generateContent' in m.supported_generation_methods:
                    name = m.name.split('/')[-1]
                    try:
                        model = genai.GenerativeModel(name)
                        model.generate_content("ping", generation_config={"max_output_tokens": 1})
                        return model
                    except: continue
        except Exception: pass
        
        # Ultimate fallback but store error info
        self.last_startup_error = last_error
        return genai.GenerativeModel('gemini-1.5-flash') # Default to flash as it's more widely available

    def test_connection(self):
        """Quickly test if the API key and model are working"""
        try:
            response = self.model.generate_content("Xin ch√†o, b·∫°n c√≥ kh·ªèe kh√¥ng?", generation_config={"max_output_tokens": 20})
            if response.text:
                return True, "K·∫øt n·ªëi th√†nh c√¥ng!"
            return False, "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI."
        except Exception as e:
            error_msg = str(e)
            if "API_KEY_INVALID" in error_msg:
                return False, "API Key kh√¥ng ch√≠nh x√°c ho·∫∑c ƒë√£ h·∫øt h·∫°n."
            elif "quota" in error_msg.lower():
                return False, "ƒê√£ h·∫øt h·∫°n m·ª©c s·ª≠ d·ª•ng (Quota) cho Key n√†y."
            return False, f"L·ªói k·∫øt n·ªëi: {error_msg}"

    def _call_ai(self, prompt):
        """Call AI via n8n or direct Gemini API"""
        # Option 1: Use n8n if configured
        if self.n8n_url:
            try:
                payload = {
                    "prompt": prompt,
                    "api_key": self.api_key
                }
                headers = {"Content-Type": "application/json"}
                response = requests.post(self.n8n_url, json=payload, headers=headers, timeout=60)
                if response.status_code == 200:
                    text = response.json().get('text', '')
                    if text: return text
                    # If empty text, fallback might be needed or return empty
                else:
                    print(f"n8n Error: {response.text}")
            except Exception as e:
                print(f"n8n Exception: {e}")
                # Fallback to local
        
        # Option 2: Direct Gemini API
        try:
            response = self.model.generate_content(prompt)
            if not response.text:
                return "‚ö†Ô∏è AI tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·ªëng. Th·ª≠ l·∫°i sau ho·∫∑c ki·ªÉm tra API Key."
            return response.text
        except Exception as e:
            error_msg = str(e)
            if "finish_reason: SAFETY" in error_msg:
                return "üõ°Ô∏è N·ªôi dung b·ªã AI ch·∫∑n do vi ph·∫°m quy t·∫Øc an to√†n. Th·ª≠ ƒë·∫∑t c√¢u h·ªèi kh√°c."
            raise e # Let the helper handle more complex errors if needed
    
    def update_context(self, **kwargs):
        """Update current context"""
        self.current_context.update(kwargs)
    
    def get_system_knowledge(self):
        """Returns string representation of key system rules for AI context"""
        knowledge = """
**KI·∫æN TH·ª®C H·ªÜ TH·ªêNG K·ª≤ M√îN:**
- H·ªá th·ªëng s·ª≠ d·ª•ng Ma tr·∫≠n Sinh Kh·∫Øc: M·ªôc sinh H·ªèa, H·ªèa sinh Th·ªï, Th·ªï sinh Kim, Kim sinh Th·ªßy, Th·ªßy sinh M·ªôc.
- C√°c cung: 1 (Kh·∫£m - Th·ªßy), 2 (Kh√¥n - Th·ªï), 3 (Ch·∫•n - M·ªôc), 4 (T·ªën - M·ªôc), 6 (C√†n - Kim), 7 (ƒêo√†i - Kim), 8 (C·∫•n - Th·ªï), 9 (Ly - H·ªèa).
- Tr·ª±c Ph√π l√† y·∫øu t·ªë l√£nh ƒë·∫°o, Tr·ª±c S·ª≠ l√† vi·ªác th·ª±c thi.
- D·ª•ng Th·∫ßn quan tr·ªçng: 
  + H√¥n nh√¢n: ·∫§t (N·ªØ), Canh (Nam), L·ª•c H·ª£p (H·ª£p t√°c).
  + Kinh doanh: Sinh M√¥n (L·ª£i nhu·∫≠n), M·∫≠u (V·ªën).
  + B·ªánh t·∫≠t: Thi√™n Nhu·∫ø (B·ªánh), Thi√™n T√¢m/·∫§t (Th·∫ßy thu·ªëc/Thu·ªëc).
- C√°c th·∫ßn: Tr·ª±c Ph√π (C√°t), ƒê·∫±ng X√† (Qu√°i d·ªã), Th√°i √Çm (M∆∞u m·∫πo), L·ª•c H·ª£p (H√≤a h·ª£p), B·∫°ch H·ªï (S√°t ph·∫°t), Huy·ªÅn V≈© (T·ªëi tƒÉm), C·ª≠u ƒê·ªãa (B·ªÅn v·ªØng), C·ª≠u Thi√™n (Ph√°t tri·ªÉn).
"""
        return knowledge

    def get_context_prompt(self):
        """Build context prompt from current state"""
        context_parts = []
        
        # Add system-wide knowledge
        context_parts.append(self.get_system_knowledge())
        
        if self.current_context.get('topic'):
            context_parts.append(f"**Ch·ªß ƒë·ªÅ hi·ªán t·∫°i:** {self.current_context['topic']}")
        
        if self.current_context.get('palace'):
            palace = self.current_context['palace']
            context_parts.append(f"**ƒêang xem cung:** Cung {palace.get('num', 'N/A')} - {palace.get('qua', 'N/A')}")
            context_parts.append(f"  - Sao: {palace.get('star', 'N/A')}")
            context_parts.append(f"  - M√¥n: {palace.get('door', 'N/A')}")
            context_parts.append(f"  - Th·∫ßn: {palace.get('deity', 'N/A')}")
        
        if self.current_context.get('dung_than'):
            context_parts.append(f"**D·ª•ng Th·∫ßn:** {', '.join(self.current_context['dung_than'])}")
        
        if self.current_context.get('last_action'):
            context_parts.append(f"**H√†nh ƒë·ªông tr∆∞·ªõc:** {self.current_context['last_action']}")
        
        if context_parts:
            return "\n".join(["**NG·ªÆ C·∫¢NH V√Ä KI·∫æN TH·ª®C HI·ªÜN T·∫†I:**"] + context_parts) + "\n\n"
        return ""
    
    def analyze_palace(self, palace_data, topic):
        """
        Analyze a specific palace with AI - WITH CONTEXT
        """
        # Update context
        self.update_context(
            topic=topic,
            palace=palace_data,
            last_action=f"Ph√¢n t√≠ch Cung {palace_data.get('num')}"
        )
        
        context = self.get_context_prompt()
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p v·ªõi ki·∫øn th·ª©c s√¢u r·ªông v·ªÅ d·ªãch h·ªçc Trung Hoa.

H√£y ph√¢n t√≠ch cung sau m·ªôt c√°ch chi ti·∫øt v√† d·ªÖ hi·ªÉu:

**Th√¥ng tin cung:**
- Cung s·ªë: {palace_data.get('num', 'N/A')}
- Qu√°i t∆∞·ª£ng: {palace_data.get('qua', 'N/A')}
- Ng≈© h√†nh: {palace_data.get('hanh', 'N/A')}
- Tinh (Sao): {palace_data.get('star', 'N/A')}
- M√¥n (C·ª≠a): {palace_data.get('door', 'N/A')}
- Th·∫ßn: {palace_data.get('deity', 'N/A')}
- Can Thi√™n: {palace_data.get('can_thien', 'N/A')}
- Can ƒê·ªãa: {palace_data.get('can_dia', 'N/A')}

**Ch·ªß ƒë·ªÅ ƒëang xem:** {topic}

H√£y ph√¢n t√≠ch theo c·∫•u tr√∫c sau:

1. **√ù nghƒ©a t·ªïng quan**: Cung n√†y ƒë·∫°i di·ªán cho ƒëi·ªÅu g√¨ trong ch·ªß ƒë·ªÅ "{topic}"?

2. **Ph√¢n t√≠ch c√°c y·∫øu t·ªë**:
   - Tinh {palace_data.get('star', 'N/A')} mang √Ω nghƒ©a g√¨?
   - M√¥n {palace_data.get('door', 'N/A')} b√°o hi·ªáu ƒëi·ªÅu g√¨?
   - Th·∫ßn {palace_data.get('deity', 'N/A')} ·∫£nh h∆∞·ªüng nh∆∞ th·∫ø n√†o?
   - T·ªï h·ª£p Can {palace_data.get('can_thien', 'N/A')}/{palace_data.get('can_dia', 'N/A')} c√≥ √Ω nghƒ©a g√¨?

3. **T∆∞∆°ng t√°c gi·ªØa c√°c y·∫øu t·ªë**: C√°c y·∫øu t·ªë n√†y k·∫øt h·ª£p v·ªõi nhau t·∫°o ra th√¥ng ƒëi·ªáp g√¨?

4. **ƒêi·ªÅm b√°o**: C√°t hay hung? M·ª©c ƒë·ªô nh∆∞ th·∫ø n√†o?

5. **L·ªùi khuy√™n c·ª• th·ªÉ**: N√™n l√†m g√¨? Tr√°nh ƒëi·ªÅu g√¨?

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß √Ω nghƒ©a."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}\n\nVui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i."
    
    def comprehensive_analysis(self, chart_data, topic, dung_than_info=None):
        """
        Comprehensive analysis with FULL CONTEXT
        """
        # Update context
        self.update_context(
            topic=topic,
            chart_data=chart_data,
            dung_than=dung_than_info or [],
            last_action="Ph√¢n t√≠ch t·ªïng h·ª£p to√†n b√†n"
        )
        
        context = self.get_context_prompt()
        
        # Build palace summary
        palace_summary = []
        for i in range(1, 10):
            palace_summary.append(f"""
Cung {i}:
- Tinh: {chart_data.get('thien_ban', {}).get(i, 'N/A')}
- M√¥n: {chart_data.get('nhan_ban', {}).get(i, 'N/A')}
- Th·∫ßn: {chart_data.get('than_ban', {}).get(i, 'N/A')}
- Can: {chart_data.get('can_thien_ban', {}).get(i, 'N/A')}/{chart_data.get('dia_can', {}).get(i, 'N/A')}
""")
        
        palaces_text = "\n".join(palace_summary)
        
        dung_than_text = ""
        if dung_than_info:
            dung_than_text = f"\n**D·ª•ng Th·∫ßn c·∫ßn ch√∫ √Ω:** {', '.join(dung_than_info)}"
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p h√†ng ƒë·∫ßu.

H√£y ph√¢n t√≠ch T·ªîNG QUAN to√†n b·ªô b√†n K·ª≥ M√¥n sau cho ch·ªß ƒë·ªÅ: **{topic}**

**Th√¥ng tin b√†n:**
{palaces_text}
{dung_than_text}

H√£y ph√¢n t√≠ch theo c·∫•u tr√∫c:

1. **T·ªïng quan t√¨nh h√¨nh** (2-3 c√¢u): Nh√¨n chung t√¨nh h√¨nh nh∆∞ th·∫ø n√†o?

2. **C√°c ƒëi·ªÉm m·∫°nh**: Nh·ªØng cung/y·∫øu t·ªë n√†o thu·∫≠n l·ª£i? T·∫°i sao?

3. **C√°c ƒëi·ªÉm y·∫øu**: Nh·ªØng cung/y·∫øu t·ªë n√†o b·∫•t l·ª£i? C·∫ßn l∆∞u √Ω g√¨?

4. **T∆∞∆°ng t√°c quan tr·ªçng**: C√≥ t∆∞∆°ng t√°c ƒë·∫∑c bi·ªát n√†o gi·ªØa c√°c cung kh√¥ng?

5. **Th·ªùi ƒëi·ªÉm**: Khi n√†o l√† th·ªùi ƒëi·ªÉm t·ªët/x·∫•u?

6. **L·ªùi khuy√™n t·ªïng h·ª£p**: 
   - N√™n l√†m g√¨?
   - Kh√¥ng n√™n l√†m g√¨?
   - Chi·∫øn l∆∞·ª£c t·ªïng th·ªÉ?

7. **D·ª± ƒëo√°n k·∫øt qu·∫£**: Kh·∫£ nƒÉng th√†nh c√¥ng? C·∫ßn chu·∫©n b·ªã g√¨?

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, c·ª• th·ªÉ v√† th·ª±c t·∫ø."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}"
    
    def answer_question(self, question, chart_data=None, topic=None):
        """
        Answer with FULL CONTEXT AWARENESS
        """
        # Use stored context if not provided
        if chart_data is None:
            chart_data = self.current_context.get('chart_data')
        if topic is None:
            topic = self.current_context.get('topic', 'Chung')
        
        # Update context
        self.update_context(
            topic=topic,
            chart_data=chart_data,
            last_action=f"H·ªèi: {question[:50]}..."
        )
        
        context = self.get_context_prompt()
        
        # Build chart context if available
        chart_context = ""
        if chart_data:
            palace_summary = []
            for i in range(1, 10):
                palace_summary.append(
                    f"Cung {i}: {chart_data.get('thien_ban', {}).get(i, 'N/A')} - "
                    f"{chart_data.get('nhan_ban', {}).get(i, 'N/A')} - "
                    f"{chart_data.get('than_ban', {}).get(i, 'N/A')}"
                )
            chart_context = "\n**B√†n K·ª≥ M√¥n hi·ªán t·∫°i:**\n" + "\n".join(palace_summary)
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p.

**B·ªëi c·∫£nh:**
- Ch·ªß ƒë·ªÅ: {topic}
{chart_context}

**C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:**
{question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n:
1. Ng·ªØ c·∫£nh hi·ªán t·∫°i (ch·ªß ƒë·ªÅ, cung ƒëang xem, h√†nh ƒë·ªông tr∆∞·ªõc)
2. Th√¥ng tin t·ª´ b√†n K·ª≥ M√¥n (n·∫øu c√≥)
3. Ki·∫øn th·ª©c v·ªÅ d·ªãch h·ªçc
4. Nguy√™n l√Ω Ng≈© h√†nh, B√°t qu√°i

Tr·∫£ l·ªùi ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, c·ª• th·ªÉ v√† th·ª±c t·∫ø b·∫±ng ti·∫øng Vi·ªát."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"
    
    def explain_element(self, element_type, element_name):
        """
        Explain element with context
        """
        # Update context
        self.update_context(
            last_action=f"Gi·∫£i th√≠ch {element_type}: {element_name}"
        )
        
        context = self.get_context_prompt()
        
        type_map = {
            'star': 'Tinh (Sao)',
            'door': 'M√¥n (C·ª≠a)',
            'deity': 'Th·∫ßn',
            'stem': 'Can (Thi√™n Can)'
        }
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p.

H√£y gi·∫£i th√≠ch chi ti·∫øt v·ªÅ {type_map.get(element_type, element_type)}: **{element_name}**

Bao g·ªìm:
1. Ngu·ªìn g·ªëc v√† √Ω nghƒ©a
2. Thu·ªôc t√≠nh (Ng≈© h√†nh, √¢m d∆∞∆°ng, v.v.)
3. T√≠nh ch·∫•t (c√°t/hung, ƒë·∫∑c ƒëi·ªÉm)
4. ·ª®ng d·ª•ng trong lu·∫≠n ƒëo√°n
5. V√≠ d·ª• c·ª• th·ªÉ

Gi·∫£i th√≠ch d·ªÖ hi·ªÉu, b·∫±ng ti·∫øng Vi·ªát."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"
